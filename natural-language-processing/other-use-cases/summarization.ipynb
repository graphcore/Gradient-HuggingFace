{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "First of all, make sure your environment has installed the latest version of [🤗 Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) as well as other dependencies:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext gc_logger` from any cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mwizak%40graphcore.ai:****@artifactory.sourcevertex.net:443/api/pypi/pypi-virtual/simple, https://pypi.python.org/simple/\n",
      "Collecting git+https://github.com/huggingface/optimum-graphcore@v0.6.1-release\n",
      "  Cloning https://github.com/huggingface/optimum-graphcore (to revision v0.6.1-release) to /tmp/pip-req-build-ctscjzc6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-graphcore /tmp/pip-req-build-ctscjzc6\n",
      "  Running command git checkout -b v0.6.1-release --track origin/v0.6.1-release\n",
      "  Switched to a new branch 'v0.6.1-release'\n",
      "  Branch 'v0.6.1-release' set up to track remote branch 'v0.6.1-release' from 'origin'.\n",
      "  Resolved https://github.com/huggingface/optimum-graphcore to commit 614e0510de01f1f66dbd73ca43b8f95905f0035d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting transformers==4.25.1 (from optimum-graphcore==0.6.1)\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting optimum==1.6.1 (from optimum-graphcore==0.6.1)\n",
      "  Using cached optimum-1.6.1-py3-none-any.whl (222 kB)\n",
      "Collecting diffusers[torch]==0.12.1 (from optimum-graphcore==0.6.1)\n",
      "  Using cached diffusers-0.12.1-py3-none-any.whl (604 kB)\n",
      "Collecting datasets (from optimum-graphcore==0.6.1)\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "Collecting tokenizers (from optimum-graphcore==0.6.1)\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting typeguard (from optimum-graphcore==0.6.1)\n",
      "  Using cached typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting sentencepiece (from optimum-graphcore==0.6.1)\n",
      "  Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting scipy (from optimum-graphcore==0.6.1)\n",
      "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting pillow (from optimum-graphcore==0.6.1)\n",
      "  Using cached Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: importlib-metadata in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (6.6.0)\n",
      "Collecting filelock (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub>=0.10.0 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting numpy (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting regex!=2019.12.17 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "Collecting requests (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (1.13.1+cpu)\n",
      "Collecting accelerate>=0.11.0 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
      "Collecting coloredlogs (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting sympy (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting transformers[sentencepiece]>=4.20.1 (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Requirement already satisfied: packaging in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.6.1) (23.1)\n",
      "Collecting numpy (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from transformers==4.25.1->optimum-graphcore==0.6.1) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from transformers==4.25.1->optimum-graphcore==0.6.1) (4.65.0)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from rouge-score) (1.16.0)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached pyarrow-12.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pandas (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached pandas-2.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Collecting xxhash (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Collecting multiprocess (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "Collecting fsspec[http]>=2021.11.1 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "Collecting aiohttp (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting responses<0.19 (from datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from typeguard->optimum-graphcore==0.6.1) (4.6.1)\n",
      "Requirement already satisfied: psutil in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from accelerate>=0.11.0->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.6.1) (23.1.0)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from importlib-metadata->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (3.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1)\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers[sentencepiece]>=4.20.1 (from optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Using cached transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "  Using cached transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "  Using cached transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "  Using cached transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "  Using cached transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "  Using cached transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
      "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "  Using cached transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "  Using cached transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "Collecting protobuf<=3.20.2 (from transformers==4.25.1->optimum-graphcore==0.6.1)\n",
      "  Using cached protobuf-3.20.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.6.1) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets->optimum-graphcore==0.6.1)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->optimum==1.6.1->optimum-graphcore==0.6.1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: optimum-graphcore\n",
      "  Building wheel for optimum-graphcore (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-graphcore: filename=optimum_graphcore-0.6.1-py3-none-any.whl size=212892 sha256=521fe478e89d1e78e22613047e77467a131a37ce6c0cc6ec3de047b5d7ed2221\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8m_f8610/wheels/5d/59/64/ea0329b77da9ec8bbc84791ba4a4c0f1e79cc5a31b1190da7f\n",
      "Successfully built optimum-graphcore\n",
      "Installing collected packages: tokenizers, sentencepiece, pytz, mpmath, xxhash, urllib3, tzdata, sympy, regex, protobuf, pillow, numpy, multidict, joblib, humanfriendly, fsspec, frozenlist, filelock, dill, click, charset-normalizer, certifi, async-timeout, absl-py, yarl, typeguard, scipy, requests, pyarrow, pandas, nltk, multiprocess, coloredlogs, aiosignal, accelerate, rouge-score, responses, huggingface-hub, aiohttp, transformers, diffusers, datasets, optimum, optimum-graphcore\n",
      "Successfully installed absl-py-1.4.0 accelerate-0.19.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 certifi-2023.5.7 charset-normalizer-3.1.0 click-8.1.3 coloredlogs-15.0.1 datasets-2.12.0 diffusers-0.12.1 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 humanfriendly-10.0 joblib-1.2.0 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.14 nltk-3.8.1 numpy-1.23.5 optimum-1.6.1 optimum-graphcore-0.6.1 pandas-2.0.1 pillow-9.5.0 protobuf-3.20.2 pyarrow-12.0.0 pytz-2023.3 regex-2023.5.5 requests-2.31.0 responses-0.18.0 rouge-score-0.1.2 scipy-1.10.1 sentencepiece-0.1.99 sympy-1.12 tokenizers-0.13.3 transformers-4.25.1 typeguard-4.0.0 tzdata-2023.3 urllib3-2.0.2 xxhash-3.2.0 yarl-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://mwizak%40graphcore.ai:****@artifactory.sourcevertex.net:443/api/pypi/pypi-virtual/simple, https://pypi.python.org/simple/\n",
      "Collecting examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable\n",
      "  Cloning https://github.com/graphcore/examples-utils (to revision latest_stable) to /tmp/pip-install-knnsj5cn/examples-utils_f22d8e043ed64c06b2f2ff94f955de95\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/graphcore/examples-utils /tmp/pip-install-knnsj5cn/examples-utils_f22d8e043ed64c06b2f2ff94f955de95\n",
      "  Running command git checkout -q 0a5fb542bee036df8583678fdcef829351081eb6\n",
      "  Resolved https://github.com/graphcore/examples-utils to commit 0a5fb542bee036df8583678fdcef829351081eb6\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting awscli>=1.24.10 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Downloading awscli-1.27.139-py3-none-any.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3>=1.26 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Downloading boto3-1.26.139-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cppimport>=22.07.17 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached cppimport-22.8.2-py3-none-any.whl\n",
      "Requirement already satisfied: filelock>=3.9.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (3.12.0)\n",
      "Collecting gitpython>=3.1 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "Collecting ipynbname>=2021.3.2 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached ipynbname-2023.1.0.0-py3-none-any.whl (4.3 kB)\n",
      "Requirement already satisfied: nbformat>=5.7.3 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (5.8.0)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (5.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (6.0)\n",
      "Collecting simple-parsing==0.0.19.post1 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Downloading simple_parsing-0.0.19.post1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect (from simple-parsing==0.0.19.post1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting botocore==1.29.139 (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Downloading botocore-1.29.139-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting docutils<0.17,>=0.10 (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "Collecting pyyaml>=5.4.1 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.29.139->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from botocore==1.29.139->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (2.8.2)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore==1.29.139->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mako (from cppimport>=22.07.17->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Collecting pybind11 (from cppimport>=22.07.17->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: ipykernel in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (6.23.1)\n",
      "Requirement already satisfied: fastjsonschema in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (2.17.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (5.9.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.19.3)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (8.12.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (8.2.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (1.5.6)\n",
      "Requirement already satisfied: packaging in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (23.1)\n",
      "Requirement already satisfied: pyzmq>=20 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (6.3.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jupyter-core->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (3.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from mako->cppimport>=22.07.17->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (2.1.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect->simple-parsing==0.0.19.post1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from typing-inspect->simple-parsing==0.0.19.post1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (4.6.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (3.15.0)\n",
      "Requirement already satisfied: backcall in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.2.0)\n",
      "Requirement already satisfied: decorator in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (4.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (6.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.139->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /nethome/mwizak/optimum_push/Gradient-HuggingFace-private/summarization_venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable) (0.2.2)\n",
      "Building wheels for collected packages: examples-utils\n",
      "  Building wheel for examples-utils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for examples-utils: filename=examples_utils-0.1.0-py3-none-any.whl size=71423 sha256=b694776261fdc13379ea2f34fd822633e63e45046fe0e93daa9fbfc95fb3af7b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ylikhzap/wheels/7b/ef/40/036b06152d165a6d3c2ade41a7f53c22e31a76a515dba9b89b\n",
      "Successfully built examples-utils\n",
      "Installing collected packages: urllib3, smmap, pyyaml, pybind11, pyasn1, mypy-extensions, mako, jmespath, docutils, colorama, typing-inspect, rsa, gitdb, cppimport, botocore, simple-parsing, s3transfer, gitpython, boto3, awscli, ipynbname, examples-utils\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.2\n",
      "    Uninstalling urllib3-2.0.2:\n",
      "      Successfully uninstalled urllib3-2.0.2\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "Successfully installed awscli-1.27.139 boto3-1.26.139 botocore-1.29.139 colorama-0.4.4 cppimport-22.8.2 docutils-0.16 examples-utils-0.1.0 gitdb-4.0.10 gitpython-3.1.31 ipynbname-2023.1.0.0 jmespath-1.0.1 mako-1.2.4 mypy-extensions-1.0.0 pyasn1-0.5.0 pybind11-2.10.4 pyyaml-5.4.1 rsa-4.7.2 s3transfer-0.6.1 simple-parsing-0.0.19.post1 smmap-5.0.0 typing-inspect-0.8.0 urllib3-1.26.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: failed to access '/root/.ipython/extensions': Permission denied\n"
     ]
    }
   ],
   "source": [
    "# %pip install \"optimum-graphcore>=0.6.0, <0.7.0\" rouge-score nltk\n",
    "%pip install git+https://github.com/huggingface/optimum-graphcore@v0.6.1-release rouge-score nltk \n",
    "%pip install examples-utils[common]@git+https://github.com/graphcore/examples-utils@latest_stable\n",
    "from examples_utils import notebook_logging\n",
    "\n",
    "# %load_ext gc_logger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62d632c377743b9bc34fca7d26a1474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt install git-lfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of Transformers and Optimum Graphcore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25.1\n",
      "0.6.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import optimum.graphcore\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(optimum.graphcore.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for machine size and cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod4\")\n",
    "executable_cache_dir = (\n",
    "    os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/summarization\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a summarization task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [🤗 Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
    "\n",
    "![Widget inference on a summarization task](images/summarization.png)\n",
    "\n",
    "We will see how to easily load the dataset for this task using 🤗 Datasets and how to fine-tune a model on it using the `IPUSeq2SeqTrainer` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a sequence-to-sequence version in the Transformers library and is supported by Optimum Graphcore. Here we picked the [`t5-small`](https://huggingface.co/t5-small) checkpoint. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/nethome/mwizak/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f703ebfd44d481e8b9918e40e5be55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_507394/2617661662.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0668c3f84c240d9826644a44886f8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n",
       " 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.',\n",
       " 'id': '35232142'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.',\n",
       " 'Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.',\n",
       " 'Trains on the west coast mainline face disruption due to damage at the Lamington Viaduct.',\n",
       " 'Many businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.',\n",
       " 'First Minister Nicola Sturgeon visited the area to inspect the damage.',\n",
       " 'The waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.',\n",
       " 'Jeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.',\n",
       " 'However, she said more preventative work could have been carried out to ensure the retaining wall did not fail.',\n",
       " '\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.',\n",
       " '\"That may not be true but it is perhaps my perspective over the last few days.',\n",
       " '\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"',\n",
       " 'Meanwhile, a flood alert remains in place across the Borders because of the constant rain.',\n",
       " 'Peebles was badly hit by problems, sparking calls to introduce more defences in the area.',\n",
       " 'Scottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.',\n",
       " \"The Labour Party's deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\",\n",
       " 'He said it was important to get the flood protection plan right but backed calls to speed up the process.',\n",
       " '\"I was quite taken aback by the amount of damage that has been done,\" he said.',\n",
       " '\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"',\n",
       " 'He said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.',\n",
       " 'Have you been affected by flooding in Dumfries and Galloway or the Borders?',\n",
       " 'Tell us about your experience of the situation and how it was handled.',\n",
       " 'Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.sent_tokenize(raw_datasets[\"train\"][0][\"document\"].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That's 70 every day. The vast majority were men.\\nThose figures do not make Japan's the highest suicide rate in the world in a developed nation.\\nThat dubious title belongs to South Korea.  But it is still far, far higher than virtually all other wealthy countries.\\nIt is three times the suicide rate in the United Kingdom.\\nThe grim self-immolation of a 71-year-old man aboard a Japanese bullet train on Tuesday has once again rammed the issue back in to the headlines here.\\nWhat drove a quiet, elderly man, to douse himself with fuel and set fire to it in a packed carriage on a speeding train?\\nAs he tipped the liquid over himself he is reported to have shooed away other passengers, telling them it was dangerous.\\nSome said there were tears in his eyes as he did so.\\nNow, as they start to dig in to his background, members of the Japanese media are turning up the tell-tale signs of a man on the edge. He lived alone and had no job. He spent his days collecting aluminium cans to sell for recycling.\\nNeighbours told reporters they had heard him smash a window after locking himself out of his dilapidated apartment.\\nOthers said they rarely saw him outside, but could often hear the sound of a television playing. Poor, old and alone. It is an all too familiar tale.\\n\"Isolation is the number one precursor for depression and suicide,\" says Wataru Nishida, a psychologist at Tokyo's Temple University.\\n\"Now it's more and more common to read stories about old people dying alone in their apartments,\" he says. \"They are being neglected. Kids used to take care of their parents in old age in Japan, but not any more.\"\\nPeople often cite Japan's long tradition of \"honourable suicide\" as a reason for the high rate here.\\nThey point to the Samurai practice of committing \"seppuku\" or to the young \"kamikaze\" pilots of 1945, to show there are distinct cultural reasons why Japanese are more likely to take their own lives.\\nTo an extent Mr Nishida agrees.\\n\"Japan has no history of Christianity,\" he says \"so here suicide is not a sin. In fact, some look at it as a way of taking responsibility.\"\\nKen Joseph from the Japan Helpline agrees. He says their experience over the last 40 years shows that elderly people who are in financial trouble may see suicide as a way out of their problems.\\n\"The insurance system in Japan is very lax when it comes to paying out for suicide,\" he says.\\n\"So when all else fails - some people feel - you can just kill yourself and the insurance will pay out.\\n\"There is sometimes an intolerable pressure on the elderly that the most loving thing they can do is take their lives and thereby provide for their family.\"\\nBecause of this, some experts think Japan's suicide rate is actually much higher than reported.\\nA lot of lone deaths of elderly people are never fully investigated by the police.\\nAccording to Ken Joseph, the almost universal practice of cremating bodies here also means that any evidence is quickly destroyed.\\nBut it is not only elderly men in financial trouble who are taking their own lives.\\nThe fastest growing suicide demographic is young men. It is now the single biggest killer of men in Japan aged 20-44.\\nAnd the evidence suggests these young people are killing themselves because they have lost hope and are incapable of seeking help.\\nThe numbers first began to rise after the Asian financial crisis in 1998. They climbed again after the 2008 worldwide financial crisis.\\nExperts think those rises are directly linked to the increase in \"precarious employment\", the practice of employing young people on short-term contracts.\\nJapan was once known as the land of lifetime employment.\\nBut while many older people still enjoy job security and generous benefits, nearly 40% of young people in Japan are unable to find stable jobs.\\nFinancial anxiety and insecurity are compounded by Japan's culture of not complaining.\\n\"There are not many ways to express anger or frustration in Japan,\" says Mr Nishida.\\n\"This is a rule-oriented society. Young people are moulded to fit in to a very small box. They have no way to express their true feelings.\\n\"If they feel under pressure from their boss and get depressed, some feel the only way out is to die.\"\\nTechnology may be making things worse, increasing young people's isolation. Japan is famous for a condition called hikikomori, a type of acute social withdrawal.\\nWhat is hikikomori?\\nMore about hikkomori\\nThe young person affected may completely shut himself - it is most often a male - off from the outside world, withdrawing in to a room and not coming out for months or even years.\\nBut that is only the most extreme form of what is now a widespread loss of direct face-to-face socialising.\\nA recent survey of young Japanese people's attitudes to relationships and sex turned up some extraordinary results. Published in January by the Japan Family Planning Association, it found that 20% of men aged 25-29 had little or no interest in having a sexual relationship.\\nWataru Nishida points to the internet and the pervasive influence of online pornography.\\n\"Young people in Japan have a lot of knowledge,\" Mr Nishida says, \"But they have no life experience. They have no idea how to express their emotions.\\n\"They have forgotten what it's like to touch a person. When they think about sex they have high anxiety and no idea how to deal with it.\"\\nAnd when young people do find themselves isolated and depressed, they have few places to turn to.\\nMental illness is still very much a taboo here. There is little popular understanding of depression. Those suffering its symptoms are often too scared to talk about it.\\nJapan's mental healthcare system is also a mess.\\nThere is an acute shortage of psychiatrists. There is also no tradition of psychiatrists working together with clinical psychologists.\\nPeople suffering from mental illness may be prescribed powerful psychotropic medicines but unlike in the West, this will often not be accompanied by a recommendation that the patient seek counselling.\\nThe counselling industry itself is a free-for-all.\\nUnlike in America or Europe, there is no government-mandated system of training and qualifying clinical psychologists.\\nAnybody can set him or herself up as a \"counsellor\" and it's very hard for someone seeking help to know whether they actually know what they are doing.\\nIt is not a happy picture, and while the suicide rate has actually begun to decline in the last three years, it is still woefully high.\\nWataru Nishida says Japan needs to start talking about mental illness much more, and not just as something scary and strange that afflicts a few.\\n\"When you see a television discussion on mental illness in Japan they still talk as if  'depression equals suicide',\" he says. \"That needs to change.\"</td>\n",
       "      <td>Last year in Japan, more than 25,000 people took their own lives.</td>\n",
       "      <td>33362387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For them, Super Tuesday could become Black Tuesday. Friday must have been gloomy enough, when Chris Christie, supposedly a card-carrying member of the establishment, kissed Donald Trump's hands and gave this political outsider his endorsement.\\nChristie's blessing came as a bolt from the blue, and taught us once more to expect the unexpected. But shouldn't the establishment - and us in the media, for that matter - have seen the billionaire coming? After all, for years the Republican standard bearers have been vulnerable to a challenge from an anti-establishment candidate.\\nBefore going on, we should say what we mean by the Republican Party establishment, a term regularly bandied around but rarely explained. Fifty years ago, it was easier to identify.\\nIt was an eastern establishment dominated by Wall Street bankers and corporate executives, who were strongly pro-business, ideologically moderate and politically pragmatic.\\nNelson Rockefeller, the scion of the banking dynasty and Governor of New York - who lived, like Donald Trump, in great splendour on Fifth Avenue - was their figurehead.\\nThese days, however, the Republican establishment is harder to define and more diffuse, which also explains why it is easier to topple.\\nMore on Trump and the Republican race for the White House:\\nAnthony Zurcher: Day one of the Republican civil war\\nThree things Donald Trump always says\\nWhat Mexicans think of Trump and his proposed wall\\nCommonly it is broadly taken to mean the Republican National Committee, senior office-holders (like Chris Christie), present and past, conservative lobbyists, like the US Chamber of Commerce, big-money donors and opinion-formers, who write for publications like the Weekly Standard, the National Review and op-ed pages of the Wall Street Journal. But that definition is open to debate. Its disparate membership explains its inability to exert control.\\nThe most obvious reason for the decline of the Republican establishment has been the rise of anti-establishment adversaries. The Tea Party, an insurgent grassroots movement that emerged after Barack Obama's inauguration, has posed the most serious threat.\\nIts hatred of the president is matched almost by its loathing for establishment Republicans in Washington, like the Senate Majority leader Mitch McConnell, who activists complain could have done more to thwart the White House.\\nTea Party primary challengers have ousted senior establishment fixtures, like Senator Richard Lugar, who represented Indiana for 36 years.\\nThe \"Hell No\" Caucus on Capitol Hill, a rump of 50 or so Tea Party-backed Republican hardliners in the House of Representatives, was strong enough to push the former House Speaker John Boehner to the point of resignation.\\nAs for opinion formers, most of the loudest and dominant voices in the modern-day conservative movement, like the talk show hosts Rush Limbaugh and Glenn Beck and the commentator Ann Coulter, are vehement critics of the establishment.\\nThe Fox News channel, even though it has often given a platform for anti-establishment voices, doesn't fall into that same category. But it has become a rival power centre, outside the control of the GOP high command.\\nGoing into the 2016 campaign, there were big clues that establishment candidates would be vulnerable. Eric Cantor, the House Republican majority leader, was ousted, unexpectedly, ahead of the 2014 congressional mid-terms. Boehner was pressured to resign as House Speaker.\\nHowever, most of us made the mistake of interpreting the results of the congressional mid-term elections as a major setback for insurgents, because they failed to make more breakthroughs.\\nTheir attempt, for example, to oust the Republican Senator Thad Cochran in Mississippi, then a six-term incumbent, was unsuccessful. In Kentucky, Mitch McConnell also crushed a Tea Party challenge in the Republican primary.\\nAccording to polls, Tea Party favourites, like Sarah Palin and Michelle Bachmann, also lost their lustre.\\nEven though the Tea Party was waning - in October last year, a Gallup poll suggested its support had dwindled to just 17% - the anger and rage that gave rise to it had not gone away.\\nConservative insurgents just needed a better candidate and more effective mouthpiece.\\nThe most obvious figure was Ted Cruz, a long-time darling of the Tea Party. But Donald Trump has proved more adept at giving voice to the politics of frustration and rage, even though he is not a Tea Party candidate per se.\\nLong before announcing his presidential bid, the billionaire had already burnished his reputation among Tea Party devotees by becoming the most prominent \"birther\" - claiming, falsely, that Barack Obama is not a natural-born citizen of the United States. His outspoken attacks on Mexicans and Muslims, combined with his contempt for political correctness, are music to insurgents' ears.\\nAnother analytical failure was to assume that the Republican establishment could do in 2016 what it has done successfully in the past seven presidential elections: to see its anointed favourite become the nominee.\\nGeorge Herbert Walker Bush, Bob Dole, George W Bush, John McCain and Mitt Romney. All were Republican establishment favourites. What's perhaps most remarkable about that run of success for the party's high command is that it continued so long.\\nWe should have paid more attention to the difficulty Mitt Romney had securing the nomination in 2012 and also the extent to which he was assisted by the absence of a strong establishment rival.\\nA central problem for the GOP high command this year, of course, has been that Marco Rubio, John Kasich, Jeb Bush and Chris Christie have split the vote.\\nNot only that, we should have been more mindful of Rick Santorum's surprise showing four years ago. The right-wing former Pennsylvania Senator won 11 states and four million votes, even though he was viewed at the outset of the race as a woefully weak candidate.\\nIt suggested that the Republican establishment would face a more serious problem in 2016 if a more compelling right-winger emerged.\\nBesides, one of the reasons why anti-establishment fervour is so strong this time round is because the grass roots is so fed up with being saddled with establishment moderates, like Romney.\\nHad we reached further back into Republican Party history we would have seen that hostile takeovers have succeeded in the past.\\nIn 1980, Reagan ran as an anti-establishment candidate, beating the blue-blood Republican George HW Bush, a scion of the establishment.\\nThen there was Barry Goldwater's success in 1964, when he scored that highly symbolic victory over Nelson Rockefeller, the great pillar of the establishment.\\nThe victory of an Arizonian right-wing firebrand over a New York moderate personified the shift in the Republican Party's centre of gravity during the civil rights era from the north-east to the south and south-west.\\nIt changed the character of the party, setting it on its present course.\\nRevulsion right now of the permanent political class and party elites seems to be a global phenomenon, but in America it is particularly pronounced, on the left as well as the right.\\nBut an anti-establishment figure like Donald Trump would not have become so strong had not the party establishment become so weak. The GOP, the Grand Old Party, has been ripe for a takeover for years.</td>\n",
       "      <td>On Super Tuesday Donald Trump's hostile takeover of the Republican Party should come even closer, and like stiff-collared executives in some wood-panelled boardroom trying belatedly to fight off a corporate raid, the GOP high command seems incapable of stopping him.</td>\n",
       "      <td>35662836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arborhill Ltd has permission to build a 100-bedroom hotel on the Hillsborough Road.\\nFilings at Companies House suggest the firm also owns an industrial property in east Belfast.\\nThe firm's last set of accounts, for 2014, show it had assets of around Â£4m and liabilities of around Â£5m.\\nIt had borrowing with the Bank of Ireland.\\nThe firm was controlled by the businessman Ken Cleland.\\nMr Cleland is a board member of the Maze Long Kesh Development Corporation</td>\n",
       "      <td>A property firm which had been planning to develop a hotel in Lisburn has been put into administration.</td>\n",
       "      <td>34626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 19-year-old, from the St Paul's club in Belfast, beat Germany's Hamza Touba on a unanimous decision to move into the flyweight quarter-finals.\\nIrvine needs to come in the top three in Istanbul to be assured of a spot on the Ireland boxing team for Rio.\\nOlympic champion Katie Taylor had an easy win over Martina Schmaranzova of the Czech Republic at the qualifiers.\\nShe will now face Yvonne Rasmussen of Denmark in the quarter-finals of the lightweight division on Wednesday.\\nCork's Christina Desmond beat top seed Nouchka Fontijan of the Netherlands at middleweight, while Ceire Smith saw off Hungary's Virginia Barankas in the flyweight division.\\nWexford's Dean Walsh suffered a split decision defeat by top seeded light-welterweight Lorenzo Sotomayor of Azerbaijan while Clonmel super-heavyweight Dean Gardiner was outpointed by Mahammadrausl Majidor, also from Azerbaijan.</td>\n",
       "      <td>Belfast boxer Brendan Irvine has cleared his first hurdle at the European Olympic qualifiers in Turkey.</td>\n",
       "      <td>36020078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The accolade was awarded by cycling's international governing body, the UCI.\\nThe Scottish event was held in June and drew a crowd of almost 20,000 people.\\nThe competition forms the third stage of the UCI World Cup Downhill championships and was first held 14 years ago.\\nThe competitions are held on a course at Nevis Range, near Fort William.\\nThis year, British downhill rider Rachel Atherton won the women's final for the ninth consecutive time.\\nSalisbury-born Atherton finished 12 seconds ahead of second-placed Tracey Hannah, from Australia.\\nManon Carpenter, from Caerphilly, South Wales, recovered from a crash to finish third.\\nSouth African Greg Minaar won the men's final. The USA's Aaron Gwin was second and Danny Hart, from Redcar, third.</td>\n",
       "      <td>This year's Fort William Mountain Bike World Cup and Buff 4X Pro Tour weekend has been named the best downhill event in the world in 2016.</td>\n",
       "      <td>38156620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lnjDIuQ3IrI-"
   },
   "source": [
    "The metric is an instance of [`datasets.Metric`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5o4rUteaIrI_",
    "outputId": "18038ef5-554c-45c5-e00a-133b02ec10f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jAWdqcUBIrJC"
   },
   "source": [
    "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6XN1Rq0aIrJC",
    "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "By default, the call above will use one of the fast tokenizers (backed by Rust) from the 🤗 Tokenizers library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "Instead of one sentence, we can pass along a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "If you are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the three arguments. `padding=\"max_length\"` will ensure that an input shorter than maximum length will be padded to the maximum length. `truncation=True` will ensure that an input longer than maximum length will be truncated to the maximum length. `max_length=max_input/target_length` sets the maximum length of a sequence.\n",
    "\n",
    "Note that it is necessary to pad all the sentences to the same length since currently Graphcore's PyTorch implementation only runs in static mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=max_input_length, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"],\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    # Since we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b70jh26IrJS",
    "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
   },
   "outputs": [],
   "source": [
    "preprocess_function(raw_datasets[\"train\"][:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the 🤗 Datasets library to avoid spending time on this step the next time you run your notebook. The 🤗 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. 🤗 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from optimum.graphcore import IPUConfig, IPUSeq2SeqTrainer, IPUSeq2SeqTrainingArguments\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "Note that  we don't get a warning like in our classification example. This means we used all the weights of the pretrained model and there is no randomly initialized head in this case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `IPUSeq2SeqTrainer`, we will need to define four more things. The first thing we need to define is the `IPUConfig`, which is a class that specifies attributes and configuration parameters to compile and put the model on the device. We initialize it with one config name or path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config_name = \"Graphcore/t5-small-ipu\"\n",
    "\n",
    "# Below, `inference_layers_per_ipu` uses the -1 wildcard\n",
    "# to split encoder and decoder layers evenly across IPUs\n",
    "# for inference\n",
    "ipu_config = IPUConfig.from_pretrained(\n",
    "    ipu_config_name,\n",
    "    executable_cache_dir=executable_cache_dir,\n",
    "    inference_layers_per_ipu=[-1],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing we need to define is the `IPUSeq2SeqTrainingArguments`, which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_batch_size = 1\n",
    "gradient_accumulation_steps = 16\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = IPUSeq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-xsum\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    per_device_eval_batch_size=micro_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    pod_type=pod_type,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=max_target_length,\n",
    "    dataloader_drop_last=True,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the three batch-size-related arguments, namely `micro_batch_size`, `gradient_accumulation_steps` and `pod_type` defined at the top of the cell and customize the weight decay. Since the `IPUSeq2SeqTrainer` will save the model regularly and our dataset is quite large, we tell it to make three saves maximum.\n",
    "\n",
    "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/t5-finetuned-xsum\"` or `\"huggingface/t5-finetuned-xsum\"`).\n",
    "\n",
    "Then, we need a special kind of data collator, which will prepare the `decoder_input_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "The last thing to define for our `IPUSeq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the `IPUSeq2SeqTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = IPUSeq2SeqTrainer(\n",
    "    model,\n",
    "    ipu_config,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Summarization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "summarization_venv",
   "language": "python",
   "name": "summarization_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
