{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# General-purpose Text Embeddings on the IPU"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook describes how to use supported embeddings models to generate SOTA text embeddings on the IPU. You can use the following:\n",
    "* [E5 model](https://arxiv.org/pdf/2212.03533.pdf) (Emb**E**ddings from bidir**E**ctional **E**ncoder r**E**presentations) to generate text embeddings on the IPU.\n",
    "* [Sentence Transformers MPNet Base V2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2), which is an embeddings model based on the MPNet base model.\n",
    "* [Sentence-T5](https://arxiv.org/abs/2108.08877), which runs on a pre-trained T5 model encoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, install the requirements for running this notebook:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Install Optimum Graphcore if it is not in your environment\n",
    "! pip install git+https://github.com/huggingface/optimum-graphcore.git \n",
    "\n",
    "! pip install sentence-transformers==2.2.2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.python.org/simple/\n",
      "Collecting git+https://github.com/huggingface/optimum-graphcore.git\n",
      "  Cloning https://github.com/huggingface/optimum-graphcore.git to /tmp/pip-req-build-2e74b3i8\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-graphcore.git /tmp/pip-req-build-2e74b3i8\n",
      "  Resolved https://github.com/huggingface/optimum-graphcore.git to commit c5a56b7011efcfef1a196257bc1fcc8c9d8b9307\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers==4.29.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (4.29.2)\n",
      "Requirement already satisfied: optimum==1.6.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (1.6.1)\n",
      "Requirement already satisfied: diffusers[torch]==0.12.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (0.12.1)\n",
      "Requirement already satisfied: cppimport==22.8.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (22.8.2)\n",
      "Requirement already satisfied: datasets in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (2.14.3)\n",
      "Requirement already satisfied: tokenizers in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (0.13.3)\n",
      "Requirement already satisfied: typeguard in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (4.1.0)\n",
      "Requirement already satisfied: sentencepiece in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (0.1.99)\n",
      "Requirement already satisfied: scipy in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (1.10.1)\n",
      "Requirement already satisfied: pillow in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum-graphcore==0.7.2.dev0) (10.0.0)\n",
      "Requirement already satisfied: mako in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from cppimport==22.8.2->optimum-graphcore==0.7.2.dev0) (1.2.4)\n",
      "Requirement already satisfied: pybind11 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from cppimport==22.8.2->optimum-graphcore==0.7.2.dev0) (2.11.1)\n",
      "Requirement already satisfied: filelock in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from cppimport==22.8.2->optimum-graphcore==0.7.2.dev0) (3.12.2)\n",
      "Requirement already satisfied: importlib-metadata in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (6.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (2023.6.3)\n",
      "Requirement already satisfied: requests in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.4 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (2.0.1+cpu)\n",
      "Requirement already satisfied: accelerate>=0.11.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (0.21.0)\n",
      "Requirement already satisfied: coloredlogs in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.7.2.dev0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.7.2.dev0) (1.12)\n",
      "Requirement already satisfied: packaging in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from optimum==1.6.1->optimum-graphcore==0.7.2.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from transformers==4.29.2->optimum-graphcore==0.7.2.dev0) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from transformers==4.29.2->optimum-graphcore==0.7.2.dev0) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore==0.7.2.dev0) (3.8.5)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from typeguard->optimum-graphcore==0.7.2.dev0) (4.7.1)\n",
      "Requirement already satisfied: psutil in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from accelerate>=0.11.0->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore==0.7.2.dev0) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from importlib-metadata->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (3.16.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (2023.7.22)\n",
      "Requirement already satisfied: networkx in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from torch>=1.4->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from torch>=1.4->diffusers[torch]==0.12.1->optimum-graphcore==0.7.2.dev0) (3.1.2)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from transformers==4.29.2->optimum-graphcore==0.7.2.dev0) (3.20.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from coloredlogs->optimum==1.6.1->optimum-graphcore==0.7.2.dev0) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from mako->cppimport==22.8.2->optimum-graphcore==0.7.2.dev0) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.7.2.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.7.2.dev0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore==0.7.2.dev0) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sympy->optimum==1.6.1->optimum-graphcore==0.7.2.dev0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum-graphcore==0.7.2.dev0) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.python.org/simple/\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (0.16.4)\n",
      "Requirement already satisfied: nltk in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: numpy in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: scipy in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (2.0.1+cpu)\n",
      "Requirement already satisfied: torchvision in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (0.15.2+cpu)\n",
      "Requirement already satisfied: tqdm in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (4.29.2)\n",
      "Requirement already satisfied: filelock in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n",
      "Requirement already satisfied: requests in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.1)\n",
      "Requirement already satisfied: sympy in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\n",
      "Requirement already satisfied: click in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.6)\n",
      "Requirement already satisfied: joblib in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from nltk->sentence-transformers==2.2.2) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from torchvision->sentence-transformers==2.2.2) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the required modules for the notebook:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "import torch\n",
    "import poptorch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import logging"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to instantiate some global parameters that will be used to run the models.\n",
    "\n",
    "The **micro batch size** (number of batches to process in parallel) is set to 2. This is smaller than usual because of the effect of the micro batch size on device memory. \n",
    "\n",
    "We use on-IPU loops (**device iterations**) which iterate over a number of batches sequentially (where the iteration takes place on-device in one dataloader call), to extend the batch size. This yields a greater throughput because it is more efficient than loading smaller batches on the host a large number of times. \n",
    "\n",
    "Data parallelism is controlled by the **replication factor**, which specifies how many devices the batch sizes are replicated over. This value is set to `None` by default as it will be automatically determined by the `pod_type` of the machine being used. By default, the model itself requires 1 IPU to run, and if it is running on a IPU-POD4 (4 IPU) machine, the replication factor is set to 4. Similarly, if the model is running on an IPU-POD16, the replication factor is set to 16. This can be overridden with a different value if needed. Specifically, if `replication_factor=N` the model will be replicated over `N` IPUs as long as `N * n_ipu (number of IPUs a single instance of the model uses) <= total available IPUs`.\n",
    "\n",
    "The total effective batch size for inference is calculated by:\n",
    "```\n",
    "effective_batch_size = replication_factor * device_iterations * micro_batch_size\n",
    "```\n",
    "\n",
    "The model itself, through model pipelining, can also be run over 4 IPUs (by setting `ipus_per_replica` to 4), in which case the replication factor will be adjusted accordingly. The reason we might want to spread the model over more IPUs is to reduce the memory consumption of the model over a single machine allowing for higher batch sizes to be used. For example, with 4 IPUs, we compute far fewer layers per IPU, while with 1 IPU, all model layers are on a single IPU. This is particularly beneficial on an IPU-POD16 machine, as the 4-IPU pipelined version of the model can be run at a higher effective batch size (with a higher micro batch size) and achieve an even higher overall batched throughput."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "logger = logging.getLogger(\"\")\n",
    "\n",
    "n_ipu = os.getenv(\"NUM_AVAILABLE_IPU\", 16)\n",
    "print(n_ipu)\n",
    "ipus_per_replica = 1\n",
    "micro_batch_size = 2\n",
    "device_iterations = 512/int(n_ipu)\n",
    "replication_factor = None\n",
    "\n",
    "random_seed = 42"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run embeddings models, we will set up a generic IPU embeddings class which loads the pre-trained model onto the IPU and runs the embedding pooling and normalisation stages in the forward pass along with the model. You may want to change the internal pooling (`pool(...)`) function in the class to support other pooling methods. The class currently supports averaging and classification using the encoder output state by passing `pool_type` when calling the model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import logging\n",
    "from typing import Optional, List\n",
    "\n",
    "from transformers import AutoModel\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from optimum.graphcore import IPUConfig\n",
    "\n",
    "logger = logging.getLogger(\"e5\")\n",
    "\n",
    "class IPUEmbeddingsModel(torch.nn.Module):\n",
    "    def __init__(self, model, ipu_config: IPUConfig, fp16=True):\n",
    "        super().__init__()\n",
    "        self.encoder = to_pipelined(model, ipu_config)\n",
    "        self.encoder = self.encoder.parallelize()\n",
    "        if fp16: self.encoder = self.encoder.half()\n",
    "    \n",
    "    def pool(self, last_hidden_states: torch.Tensor, attention_mask: torch.Tensor, pool_type: str) -> torch.Tensor:\n",
    "             \n",
    "        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    \n",
    "        if pool_type == \"avg\":\n",
    "            emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "        elif pool_type == \"cls\":\n",
    "            emb = last_hidden[:, 0]\n",
    "        else:\n",
    "            raise ValueError(f\"pool_type {pool_type} not supported\")\n",
    "\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, pool_type: str ='avg', **kwargs) -> torch.Tensor:\n",
    "        outputs = self.encoder(**kwargs)\n",
    "        \n",
    "        embeds = self.pool(outputs.last_hidden_state, kwargs[\"attention_mask\"], pool_type=pool_type)\n",
    "        embeds = torch.nn.functional.normalize(embeds, p=2, dim=-1)\n",
    "\n",
    "        return embeds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before setting up and running each of the models, let's create a simple `infer` function which handles loading the batches from a dataloader and generating the embeddings from any model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import time\n",
    "\n",
    "def infer(model, dataloader):\n",
    "    encoded_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_dict in tqdm(dataloader, desc='encoding'):\n",
    "            lat = time.time()\n",
    "            outputs = model(**batch_dict)\n",
    "            lat = time.time() - lat\n",
    "            \n",
    "            encoded_embeds.append(outputs)\n",
    "            print(f\"batch len: {len(batch_dict['input_ids'])} | batch latency: {lat}s | per_sample: {lat/len(batch_dict['input_ids'])}s | throughput: {len(batch_dict['input_ids'])/lat} samples/s\")\n",
    "    \n",
    "    return torch.cat(encoded_embeds, axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings with E5-Large"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, use `AutoConfig` from Transformers to load the model config for the E5 large model. E5 uses a bidirectional encoder, essentially the encoder stage of a BERT model, to generate the trained embeddings. The config will define the architecture of the model, such as the number of encoder layers and size of the hidden dimension within the model. The sequence length for the model is set by default to the maximum defined sequence length in the model config (`max_position_embeddings`) and can be adjusted by changing the `e5_seq_len` parameter, with a maximum value of 512.\n",
    "\n",
    "We also need to tokenize the dataset. For this we define a custom transform function which applies the pre-trained tokenization for each model to the dataset. We will call this function when loading the function, to avoid loading multiple tokenized datasets at the same time.\n",
    "\n",
    "We define some IPU-specific configurations to get the most out of the model. The `get_ipu_config` function will set up the IPU config according to the model config, taking into consideration the defined number of IPUs for model parallelism, the number of IPUs available and batching configurations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from config import get_ipu_config\n",
    "from transformers import AutoConfig, AutoTokenizer, BatchEncoding"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "e5_model_name = 'intfloat/e5-large'\n",
    "e5_tokenizer = AutoTokenizer.from_pretrained(e5_model_name)\n",
    "e5_model_config = AutoConfig.from_pretrained(e5_model_name)\n",
    "e5_model = AutoModel.from_pretrained(e5_model_name, config=e5_model_config)\n",
    "\n",
    "e5_seq_len = e5_model_config.max_position_embeddings\n",
    "\n",
    "def e5_transform_func(example) -> BatchEncoding:\n",
    "    return e5_tokenizer(\n",
    "        example['text'],\n",
    "        max_length = e5_seq_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "e5_ipu_config = get_ipu_config(\n",
    "    e5_model_config, n_ipu, ipus_per_replica, device_iterations, replication_factor, random_seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings with All-MPNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For MPNet, we do the same for the pre-trained model. The maximum value for the sequence length is 512."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "mpnet_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "mpnet_tokenizer = AutoTokenizer.from_pretrained(mpnet_model_name)\n",
    "mpnet_model_config = AutoConfig.from_pretrained(mpnet_model_name)\n",
    "mpnet_model = AutoModel.from_pretrained(mpnet_model_name, config=mpnet_model_config)\n",
    "\n",
    "mpnet_seq_len = mpnet_model_config.max_position_embeddings\n",
    "\n",
    "def mpnet_transform_func(example) -> BatchEncoding:\n",
    "    return mpnet_tokenizer(\n",
    "        example['text'],\n",
    "        max_length=mpnet_seq_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "mpnet_ipu_config = get_ipu_config(\n",
    "    mpnet_model_config, n_ipu, ipus_per_replica, device_iterations, replication_factor, random_seed)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5812cb40bd7442fb89f2c7bd0831530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771911e251304ded8d307bf741dc0f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688c4f553848471f9e63a6faa27f9642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5616e96cb1a94da5ad5a3f966393eeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd22f48177a042c18d35d906c744fddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168135a4fbda40969d36789de13e8aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979f7971cb7d4fccb51be45127a19da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)main/ipu_config.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embeddings with Sentence-T5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that for T5, we need to use `T5EncoderModel` instead of `AutoModel`. We must manually specify the encoder as T5 is an encoder-decoder model, and we don't want to load the decoder for embeddings generation. \n",
    "\n",
    "Transformers `AutoModel` supports a 1-to-1 mapping of architecture definitions to model types, and it will load the `T5Model` class by default. We can override this by directly importing and loading the pre-trained model using `T5EncoderModel`. For T5 the sequence length is determined by the `n_positions` parameter in the model config. The expected maximum sequence length for T5 is also 512."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from transformers.models.t5.modeling_t5 import T5EncoderModel\n",
    "\n",
    "t5_model_name = 'sentence-transformers/sentence-t5-base'\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "t5_model_config = AutoConfig.from_pretrained(t5_model_name)\n",
    "t5_model = T5EncoderModel.from_pretrained(t5_model_name, config=t5_model_config)\n",
    "\n",
    "t5_seq_len = t5_model_config.n_positions\n",
    "\n",
    "def t5_transform_func(example) -> BatchEncoding:\n",
    "    return t5_tokenizer(\n",
    "        example['text'],\n",
    "        max_length=t5_seq_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "t5_ipu_config = get_ipu_config(\n",
    "    t5_model_config, n_ipu, ipus_per_replica, device_iterations * 4, replication_factor, random_seed)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578f5134d6ac4fd3b0546f9f6cb87363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d471f698f1fa4d7f9b723958445946d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27be64b2bdd744d795d3bebc0d94f2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5dbdd036d45f58705bae2b77b14f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b45042038a4fe9a91068cf179997cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589bfd858b8746e4a4b3b32ae4bd4a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/219M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`replicated_tensor_sharding` is not used when `replication_factor=1`\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the embeddings model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll wrap this behaviour into a simple function so we can iteratively run all three models and initialise `poptorch.Dataloader` to create an IPU-ready batched dataloader. We pass an arbitrary call to the model using the first batch to ensure we have compiled the model executable (or loaded the already compiled executable).\n",
    "\n",
    "The function goes through the model and dataset setup for a given model and:\n",
    "1. Initialises the `IPUEmbeddingsModel` class with the loaded model and IPU config.\n",
    "2. Converts the IPU config into an IPU options object and passes this to a `poptorch.inferenceModel` wrapper to prepare the model for the IPU.\n",
    "3. Initialises [`poptorch.Dataloader`](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html) to batch the data according to the IPU options and the defined micro batch size.\n",
    "4. Runs the model once with a batch to compile or load the compiled executable. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from transformers import default_data_collator as data_collator\n",
    "\n",
    "def create_model(model, ipu_config, dataset, micro_batch_size):\n",
    "    model = IPUEmbeddingsModel(model, ipu_config)\n",
    "\n",
    "    ipu_options = ipu_config.to_options(for_inference=True)\n",
    "    model = poptorch.inferenceModel(model, ipu_options)\n",
    "\n",
    "    dataloader = poptorch.DataLoader(\n",
    "        ipu_options,\n",
    "        dataset['train'],\n",
    "        batch_size=micro_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    model(**next(iter(dataloader)))\n",
    "    return model, dataloader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load a dataset we'll use to try out the models. Using the Hugging Face `datasets` library we can load a pre-existing dataset from the Hugging Face Hub. In this case, let's use the `rotten_tomatoes` film review dataset. Later in the notebook, we will use this dataset to create a basic semantic search functionality."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be149fe51d674ce9b6501409706c81ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0588a63e4ffd42a7aadfd22cfd045537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7847fe32a544e2b894794d07a7d0bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23ef20d6e9949779dad9bfa159947dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c428e90f08e547db9cb749577daddd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95adc9d18a694552becc6d4b44bae739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dda0dc87564cb7ab5e666046062173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the E5 model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset first needs to be tokenized using the pre-trained tokenizer for each model, we can use the `map()` method to tokenize each of the inputs of the dataset using the model-specific transform function. Then we can convert the Hugging Face Arrow format dataset to a PyTorch-ready dataset with `set_format` which converts the tokenized inputs into tensors.\n",
    "\n",
    "To run the model, simply call the `infer` function we created earlier to generate embeddings for the full dataset. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "tokenized_dataset = dataset.map(e5_transform_func, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "print(e5_ipu_config)\n",
    "\n",
    "model, dataloader = create_model(e5_model, e5_ipu_config, tokenized_dataset, micro_batch_size)\n",
    "\n",
    "e5_data_embeddings = infer(model, dataloader)\n",
    "\n",
    "model.detachFromDevice()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IPUConfig {\n",
      "  \"auto_loss_scaling\": false,\n",
      "  \"device_iterations\": 1,\n",
      "  \"embedding_serialization_factor\": 2,\n",
      "  \"enable_half_partials\": true,\n",
      "  \"executable_cache_dir\": \"./exe_cache/\",\n",
      "  \"execute_encoder_on_cpu_for_generation\": false,\n",
      "  \"explicit_ir_inference\": false,\n",
      "  \"gradient_accumulation_steps\": 512,\n",
      "  \"inference_device_iterations\": 128,\n",
      "  \"inference_embedding_serialization_factor\": 1,\n",
      "  \"inference_ipus_per_replica\": 1,\n",
      "  \"inference_layers_per_ipu\": [\n",
      "    24\n",
      "  ],\n",
      "  \"inference_matmul_proportion\": [\n",
      "    0.1\n",
      "  ],\n",
      "  \"inference_parallelize_kwargs\": {},\n",
      "  \"inference_projection_serialization_factor\": 1,\n",
      "  \"inference_replication_factor\": 16,\n",
      "  \"inference_serialized_embedding_splits_per_ipu\": null,\n",
      "  \"inference_serialized_projection_splits_per_ipu\": null,\n",
      "  \"ipus_per_replica\": 4,\n",
      "  \"layers_per_ipu\": [\n",
      "    3,\n",
      "    7,\n",
      "    7,\n",
      "    7\n",
      "  ],\n",
      "  \"matmul_proportion\": [\n",
      "    0.1,\n",
      "    0.15,\n",
      "    0.15,\n",
      "    0.15\n",
      "  ],\n",
      "  \"optimizer_state_offchip\": true,\n",
      "  \"optimum_version\": \"1.6.1\",\n",
      "  \"output_mode\": \"final\",\n",
      "  \"parallelize_kwargs\": {},\n",
      "  \"projection_serialization_factor\": 1,\n",
      "  \"recompute_checkpoint_every_layer\": false,\n",
      "  \"replicated_tensor_sharding\": true,\n",
      "  \"replication_factor\": 16,\n",
      "  \"seed\": 42,\n",
      "  \"serialized_embedding_splits_per_ipu\": null,\n",
      "  \"serialized_projection_splits_per_ipu\": null,\n",
      "  \"transformers_version\": \"4.29.2\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "No IPU hardware available on this system",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenized_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(e5_ipu_config)\n\u001b[0;32m----> 5\u001b[0m model, dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43me5_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me5_ipu_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmicro_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m e5_data_embeddings \u001b[38;5;241m=\u001b[39m infer(model, dataloader)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mdetachFromDevice()\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model, ipu_config, dataset, micro_batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_model\u001b[39m(model, ipu_config, dataset, micro_batch_size):\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m IPUEmbeddingsModel(model, ipu_config)\n\u001b[0;32m----> 6\u001b[0m     ipu_options \u001b[38;5;241m=\u001b[39m \u001b[43mipu_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfor_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m poptorch\u001b[38;5;241m.\u001b[39minferenceModel(model, ipu_options)\n\u001b[1;32m      9\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m poptorch\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     10\u001b[0m         ipu_options,\n\u001b[1;32m     11\u001b[0m         dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         collate_fn\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     17\u001b[0m     )\n",
      "File \u001b[0;32m/localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/optimum/graphcore/ipu_configuration.py:688\u001b[0m, in \u001b[0;36mIPUConfig.to_options\u001b[0;34m(self, for_inference, compile_only)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, for_inference: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, compile_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m poptorch\u001b[38;5;241m.\u001b[39mOptions:\n\u001b[1;32m    676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03m    Creates a `poptorch.Options` instance from the `IPUConfig` instance.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;124;03m        `poptorch.Options`: The options representing the `IPUConfig` instance.\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfor_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfor_inference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_only\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/optimum/graphcore/ipu_configuration.py:559\u001b[0m, in \u001b[0;36mIPUConfig._to_options\u001b[0;34m(self, for_inference, compile_only)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, for_inference: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, compile_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m poptorch\u001b[38;5;241m.\u001b[39mOptions:\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compile_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpoptorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mipuHardwareVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m21\u001b[39m):\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis requires an IPU Mk2 system to run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_encoder_on_cpu_for_generation:\n",
      "File \u001b[0;32m/localdata/evaw/evaw/venvs/3.3.0+1403/3.3.0+1403_poptorch/lib/python3.8/site-packages/poptorch/__init__.py:768\u001b[0m, in \u001b[0;36mipuHardwareVersion\u001b[0;34m()\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Indicates what IPU hardware version is available in the system.\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mRaise an exception if no hardware is available.\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m:returns: The IPU hardware version or -1 if unknown.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m version \u001b[38;5;241m=\u001b[39m poptorch_core\u001b[38;5;241m.\u001b[39mipuHardwareVersion()\n\u001b[0;32m--> 768\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m version \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo IPU hardware available on this system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m version\n",
      "\u001b[0;31mAssertionError\u001b[0m: No IPU hardware available on this system"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the All-MPNet model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_dataset = dataset.map(mpnet_transform_func, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "model, dataloader = create_model(mpnet_model, mpnet_ipu_config, tokenized_dataset, micro_batch_size)\n",
    "\n",
    "mpnet_data_embeddings = infer(model, dataloader)\n",
    "\n",
    "model.detachFromDevice()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run the Sentence-T5 model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized_dataset = dataset.map(t5_transform_func, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "model, dataloader = create_model(t5_model, t5_ipu_config, tokenized_dataset, micro_batch_size)\n",
    "\n",
    "t5_data_embeddings = infer(model, dataloader)\n",
    "\n",
    "model.detachFromDevice()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embeddings for a single sequence represent low-dimensional numerical representations of the word-level and sentence-level context for each token. These pre-trained embeddings can be used in applications like embedding retrieval for recommender systems, or semantic searches for query-matching using cosine-similarity. Both of these use cases take advantage of the generated embeddings space, by performing a relative comparison of the user input sequence embeddings using some proximity metric.\n",
    "\n",
    "We'll use the open source `sentence_transformers` library which provides utilities for embeddings tasks to perform a semantic search on a user query to retrieve the sequences from the dataset that are most similar to the query. This is a helpful utility for making, for example, more responsive FAQs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantic search with generated embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the `rotten_tomatoes` dataset, lets create a simple similarity search engine using the `sentence_transformers` semantic search function, which uses cosine similarity to retrieve close-proximity sentences from a given set of embeddings to a given query. We have already generated embeddings for the dataset, so the next step is to do the same with a given query and perform the search.\n",
    "\n",
    "First, to process the query, we need to tokenize it and convert it to a single-batch input for the model. This has been wrapped into a simple function which tokenizes and prepares a dictionary of model inputs (`input_ids`, `attention_mask`, ...) to which we just need to pass a string."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare_query(query: str):\n",
    "    t_query = mpnet_tokenizer(\n",
    "            query,\n",
    "            max_length=mpnet_model_config.max_position_embeddings,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    return {k: torch.as_tensor([t_query[k]]) for k in t_query}"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, to perform inference with a single input (so an effective batch size of 1) we re-instantiate the model by setting all device batching, replication and micro batch-size to 1 and re-compile the model. For this example, we use the All-MPNet model. The change in batch size necessitates a recompilation, since the input shape to the model has been changed. We will follow the steps to initiate the model outlined earlier in the notebook, with the only change being setting the `get_ipu_config` function to have all batching turned off."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mpnet_infer_ipu_config = get_ipu_config(\n",
    "    mpnet_model_config, n_ipu, ipus_per_replica=1, device_iterations=1, replication_factor=1, random_seed=random_seed)\n",
    "\n",
    "model = IPUEmbeddingsModel(mpnet_model, mpnet_infer_ipu_config)\n",
    "model = poptorch.inferenceModel(model, mpnet_infer_ipu_config.to_options(for_inference=True))\n",
    "\n",
    "o=model(**prepare_query(\"Running once to compile\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can use the model to embed a single query, and perform a semantic search across the full dataset embeddings to retrieve highly relevant reviews to the query."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "query = \"Strongly disliked this action movie\"\n",
    "\n",
    "query_embeddings = model(**prepare_query(query))\n",
    "hits = semantic_search(query_embeddings.float(), mpnet_data_embeddings.float(), top_k=10)\n",
    "\n",
    "print(f\"\\n SEARCH QUERY: {query}\")\n",
    "for n, res in enumerate(hits[0]):\n",
    "    print(f\"\\n Result (rank {n+1}) | Score: {res['score']} | Text: {dataset['train']['text'][res['corpus_id']]} \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.detachFromDevice()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}