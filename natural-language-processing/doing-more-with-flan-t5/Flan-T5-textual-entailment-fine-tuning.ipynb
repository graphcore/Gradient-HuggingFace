{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd.\n",
    "\n",
    "# Textual Entailment on IPUs using Flan-T5 - Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) is an encoder-decoder transformer model that reframes all NLP tasks into a text-to-text format. Compared to T5, Flan-T5 has been fine-tuned on more than 1000 additional tasks. It can also be used for text generation.\n",
    "\n",
    "Language models are very powerful because a huge variety of tasks can be formulated as text-to-text problems and thus adapted to fit the generative setup, where the model is asked to correctly predict future tokens. For more details, check out the T5 paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf), and the Flan-T5 paper [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we apply the idea from the T5 paper and fine-tune Flan-T5 on the task of textual entailment with the [GLUE MNLI](https://huggingface.co/datasets/glue#mnli) dataset.\n",
    "\n",
    "We also show how you can easily adapt the example in this notebook for custom fine-tuning of several downstream tasks, such as question answering, named entity recognition, sentiment analysis and text classification. All you would need to do is prepare the data in the way needed for the specific task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Domain | Tasks | Model | Datasets | Workflow |   Number of IPUs   | Execution time |\n",
    "|---------|-------|-------|----------|----------|--------------|--------------|\n",
    "|   NLP   |  Textual entailment  | Flan-T5 | Glue-MNLI| Fine-tuning | recommended: 64 (min: 16) |  4h (with 16 IPUs: 7h)  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning outcomes\n",
    "In this demo, you will:\n",
    "- Prepare a configuration for fine-tuning and one for validation, and optionally customise them according to your needs\n",
    "- Download and preprocess the MNLI dataset in a way suitable for the T5 architecture\n",
    "- Fine-tune the Flan-T5 model on the dataset, and then perform validation in order to compute the achieved accuracy\n",
    "- Save a Hugging Face checkpoint of the fine-tuned model, suitable for use on any hardware\n",
    "- Learn how to adapt this notebook to any downstream task, exploiting the generality of the text-to-text approach of T5\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "[![Run on Gradient](../../../gradient-badge.svg)](https://ipu.dev/IotDe5)\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to do this. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine.\n",
    "\n",
    "## Dependencies and configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "This notebook supports both Flan-T5 XXL (11B parameters) and Flan-T5 XL (3B parameters) and the code below refers to the XXL model. Note: the XXL variant needs a minimum of 16 IPUs to run, while the XL variant needs a minimum of 8 IPUs. Handling the weights and checkpoints of XXL is going to take a bit more time compared to the XL variant. So if you want to run quick experiments, you can change the code below to use the XL model.\n",
    "\n",
    "Note that if you have enough IPUs available, you can speed up training by using data parallelism. We'll see how to do this in the section [Data parallel](#data-parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following to \"xl\" if you want to use the smaller variant\n",
    "model_size = \"xxl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ipus_needed = 16 if model_size == \"xxl\" else 8\n",
    "number_of_ipus = int(os.getenv(\"NUM_AVAILABLE_IPU\", ipus_needed))\n",
    "if number_of_ipus < ipus_needed:\n",
    "    raise ValueError(\n",
    "        f\"This example needs {ipus_needed} IPUs to work. Detected {number_of_ipus}\"\n",
    "    )\n",
    "\n",
    "os.environ[\"POPART_CACHE_DIR\"] = os.getenv(\n",
    "    \"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache/\"\n",
    ")\n",
    "checkpoint_dir = os.getenv(\"CHECKPOINT_DIR\", \"checkpoints\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning configuration\n",
    "First of all, we need to load a default configuration, defined in `config/finetuning.yml`.\n",
    "This file has optimised configurations to run the model on IPUs.\n",
    "We need to pick the configuration suitable for our model size.\n",
    "\n",
    "This configuration uses a sequence length of 512 tokens. Flan-T5 XXL layers are split across 16 IPUs, using [tensor model parallelism](https://arxiv.org/pdf/1909.08053.pdf). No data parallelism is used by default.\n",
    "The `t5_config_setup` sets up the specified configuration and configures logging and Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.setup import t5_config_setup\n",
    "from config import CONFIG_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We support logging to Weights & Biases. If you want to use it, you first need to manually log in (see the [W&B quickstart](https://docs.wandb.ai/quickstart) for details).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you want to use W&B. Be sure to be logged in.\n",
    "wandb_setup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a configuration\n",
    "config_name = \"xxl_pod16\" if model_size == \"xxl\" else \"xl_pod8\"\n",
    "config, args, _ = t5_config_setup(\n",
    "    CONFIG_DIR / \"finetuning.yml\", \"release\", config_name, wandb_setup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.dumps_yaml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation configuration\n",
    "Configurations for inference are found in `config/inference.yml`:\n",
    "- `xxl` is the default configuration, it uses a batch size of 12 and it's guaranteed to fit into memory with a 512 sequence length.\n",
    "- `xxl-mnli` is optimised for the MNLI dataset. This increases the batch size to 20 but requires the sequence length to be reduced to 268.\n",
    "\n",
    "In this example we will start with the default configuration, and manually reduce the sequence length and increase the batch size later on.\n",
    "You can do the same on your custom dataset to find the best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True if you want to use W&B. Be sure to be logged in.\n",
    "wandb_setup_on_eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config, *_ = t5_config_setup(\n",
    "    CONFIG_DIR / \"inference.yml\",\n",
    "    \"release\",\n",
    "    model_size,\n",
    "    hf_model_setup=False,\n",
    "    wandb_setup=wandb_setup_on_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_config.dumps_yaml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The MNLI dataset consists of pairs of sentences, a *premise* and a *hypothesis*.\n",
    "The task is to predict the relation between the premise and the hypothesis, which can be:\n",
    "- `entailment`: hypothesis follows from the premise,\n",
    "- `contradiction`: hypothesis contradicts the premise,\n",
    "- `neutral`: hypothesis and premise are unrelated.\n",
    "\n",
    "Data splits for the MNLI dataset are the following:\n",
    "\n",
    "|train |validation_matched|validation_mismatched|\n",
    "|-----:|-----------------:|--------------------:|\n",
    "|392702|              9815|                 9832|\n",
    "\n",
    "The matched split is made of samples derived from the same sources as those in the training set, and samples in the mismatched split are not derived from the same sources as those in the training set and therefore don't closely resemble any of the examples seen at training time. For validation we're going to use the latter.\n",
    "You can explore it on [Hugging Face](https://huggingface.co/datasets/glue/viewer/mnli/train).\n",
    "![MNLI dataset](imgs/mnli_dataset.png)\n",
    "\n",
    "\n",
    "### Training preprocessing\n",
    "The columns we are interested in are `hypothesis`, `premise` and `label`.\n",
    "\n",
    "As mentioned at the beginning, T5 has an encoder-decoder architecture, so it needs 2 input sequences: one for the encoder, and one for the decoder.\n",
    "So, the first step consists of forming input prompts for the encoder with the format\n",
    "```\n",
    "mnli hypothesis: {hypothesis} premise: {premise}\n",
    "```\n",
    "Next, we provide the decoder with the corresponding label, shifted right and prepended with the `<pad>` token:\n",
    "```\n",
    "<pad>{label}\n",
    "```\n",
    "For example, an encoder sequence would be:\n",
    "```\n",
    "mnli hypothesis: Product and geography are what make cream skimming work.  premise: Conceptually cream skimming has two basic dimensions - product and geography.\n",
    "```\n",
    "Similarly, an example decoder sequence would be:\n",
    "```\n",
    "<pad>neutral\n",
    "```\n",
    "The pad token acts as `decoder_start_token_id` for the T5 models.\n",
    "\n",
    "Then, the encoder and decoder sequences are tokenized and padded to the model sequence length of 512. Attention masks are generated accordingly.\n",
    "Since the model is trained to predict the `mnli` class, the labels are simply the decoder input sequence shifted by one token to the left, which means that the labels will simply be the `mnli` class, without the pad token at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, disable_progress_bar, enable_progress_bar\n",
    "from transformers import AutoTokenizer\n",
    "import data.mnli_data as mnli_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells are the ones you want to change when doing custom fine-tuning.\n",
    "\n",
    "We first load the MNLI dataset, and then create a custom preprocessing function to build prompts suitable for a\n",
    "text-to-text setup.\n",
    "For a custom fine-tuning, you will need to choose a format for your prompts and change the `form_training_prompts` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF dataset\n",
    "dataset = load_dataset(\"glue\", \"mnli\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form prompts in the format: \"mnli hypothesis: {hypothesis} premise: {premise}\"\n",
    "def form_training_prompts(example):\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    premise = example[\"premise\"]\n",
    "    class_label = [\"entailment\", \"neutral\", \"contradiction\"][example[\"label\"]]\n",
    "\n",
    "    example[\"text\"] = f\"mnli hypothesis: {hypothesis} premise: {premise}\"\n",
    "    example[\"target\"] = f\"{class_label}\"\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    form_training_prompts,\n",
    "    remove_columns=[\"hypothesis\", \"premise\", \"label\", \"idx\"],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Generating text prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows first textual prompt\n",
    "dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we tokenize the prompts. You won't need to change this step for custom fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts\n",
    "# We disable the progress bar for this,\n",
    "# because when num_proc > 1 it's a bit messy\n",
    "disable_progress_bar()\n",
    "dataset = dataset.map(\n",
    "    mnli_data.tokenizes_text(tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=8,\n",
    "    remove_columns=dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Tokenizing text\",\n",
    ")\n",
    "enable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows first tokenized prompt\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a portion of first decoder input. You can see that the decoder input is the label prepended with the pad token (0).\n",
    "print(\"First 10 tokens of first decoder input\")\n",
    "print(\"decoder_input_ids\")\n",
    "print(dataset[0][\"decoder_input_ids\"][:10])\n",
    "# Note that the labels' tokens that correspond to padding have been set to -100,\n",
    "# this value will be ignored when computing the cross-entropy loss\n",
    "print(\"labels\")\n",
    "print(dataset[0][\"labels\"][:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** If you want to adapt this code for another dataset, be sure to call all the inputs and labels in the same way: `input_ids`, `attention_mask`, `decoder_input_ids`, `decoder_attention_mask` and `labels`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation preprocessing\n",
    "For validation, we use the [MNLI `validation_mismatched`](https://huggingface.co/datasets/glue/viewer/mnli_mismatched/validation) split.\n",
    "\n",
    "Similar to what we did for training, the first preprocessing step is creating prompts for the encoder inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(\"glue\", \"mnli\", split=\"validation_mismatched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_validation_prompts(example):\n",
    "    hypothesis = example[\"hypothesis\"]\n",
    "    premise = example[\"premise\"]\n",
    "\n",
    "    example[\"text\"] = f\"mnli hypothesis: {hypothesis} premise: {premise}\"\n",
    "    return example\n",
    "\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    form_validation_prompts,\n",
    "    remove_columns=[\"hypothesis\", \"premise\", \"idx\"],\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Generating text prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, input prompts are tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(tokenizer):\n",
    "    def func(dataset):\n",
    "        tokenized = tokenizer(\n",
    "            dataset[\"text\"], padding=\"max_length\", return_tensors=\"np\"\n",
    "        )\n",
    "        tokenized.update(label=dataset[\"label\"])\n",
    "        return tokenized\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    prepare_validation_features(tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Tokenizing text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note** If you want to adapt this code for another dataset, be sure to call all the inputs in the same way: `input_ids`, `attention_mask` and `labels`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customise configuration and create a Trainer instance\n",
    "Right at the beginning of the notebook we loaded the default configurations for training and inference. We are now going to show how to customise some of the parameters for your needs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise training configuration\n",
    "In the cells below we list the parameters you are most likely to change when doing a custom fine-tuning.\n",
    "\n",
    "These are the training steps, learning rate and optimizer parameters.\n",
    "\n",
    "Moreover, it is important that you specify **checkpoint** parameters, namely a folder to save the fine-tuned weights and a periodicity for checkpointing. Be aware that saving checkpoints takes time, so you don't want to save them too often.\n",
    "To disable intermediate checkpoints set `config.checkpoint.steps = 0`. The final checkpoint is always saved provided the `config.checkpoint.save` directory is given. Set it to `None` if you don't want to save weights, but it's unlikely that you want to disable the last checkpoint.\n",
    "\n",
    "If you are not resuming training and you don't care about resuming the training later on you can reduce the time and memory required to save checkpoints by specifying `optim_state=False`. In this case, only the model weights will be saved, while the optimiser state will be discarded.\n",
    "\n",
    "Checkpoints will be saved in the directory given by the environment variable `CHECKPOINT_DIR`, which we saved in `checkpoint_dir` at the beginning. You can have a look at the path by printing it out in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise training arguments\n",
    "# Note that for the XXL variant it will take approximately 36 seconds per training step,\n",
    "# and the default number of steps is 500, which means it will take\n",
    "# about 5 hours to train.\n",
    "# For the XL variant it will take approximately 16 seconds\n",
    "# per training step, so about 2.25 hours for 500 steps.\n",
    "# Here you can set a lower number of steps, to finish training earlier.\n",
    "# NOTE: you need to increase the following to 500 in order to reach 87% accuracy.\n",
    "config.training.steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise optimiser and learning rate schedule\n",
    "config.training.optimizer.learning_rate.maximum = 5e-6\n",
    "config.training.optimizer.learning_rate.warmup_steps = 10\n",
    "config.training.optimizer.beta1 = 0.9\n",
    "config.training.optimizer.beta2 = 0.999\n",
    "config.training.optimizer.weight_decay = 0.0\n",
    "config.training.optimizer.gradient_clipping = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customise checkpoints\n",
    "config.checkpoint.save = (\n",
    "    checkpoint_dir  # where the model is saved. None means don't save any checkpoint\n",
    ")\n",
    "config.checkpoint.steps = (\n",
    "    0  # how often you save the model. 0 means only the final checkpoint is saved\n",
    ")\n",
    "config.checkpoint.to_keep = 4  # maximum number of checkpoints kept on disk\n",
    "config.checkpoint.optim_state = (\n",
    "    False  # Whether to include the optimiser state in checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training\n",
    "config.checkpoint.load = (\n",
    "    None  # you can specify a directory containing a previous checkpoint\n",
    ")\n",
    "# os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data parallel\n",
    "If you have enough IPUs available, you can speed up training by using data parallelism. As mentioned previously, the XXL variant needs 16 IPUs, so if you have 64 IPUs you can set data parallel to 4. Similarly, the XL variant needs 8 IPUs, so if you have 16 IPUs you can set data parallel to 2 (or to higher values if you have more than 16 IPUs).\n",
    "\n",
    "Note that when changing the data parallel value the model will need to be re-compiled the first time it is run with the changed parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the following to a suitable value that depends on\n",
    "# the number of IPUs in your system and the model variant you chose\n",
    "config.execution.data_parallel = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customise validation configuration\n",
    "For validation there are fewer relevant parameters.\n",
    "\n",
    "You can control the maximum number of tokens generated by the model using the  `output_length` parameter. If you know that the targets are only a few tokens long, it is convenient to set it to a small number. Generation stops if the output token is `</s>` (the end-of-text token) or after `output_length` tokens are generated.\n",
    "\n",
    "In our case, we set the `output_length` to 5 to accommodate all class labels and the `</s>` token.\n",
    "\n",
    "You can also reduce the model `sequence_length` to account for the maximum length of sentences encountered in the validation dataset.\n",
    "In our case of the MNLI example, `sequence_length = 268` and this allows us to increase the batch size to `20`.\n",
    "\n",
    "If you adapt this example to another dataset, you can compare your dataset `max_len` with ours and decide if you can safely use the increased batch size. Otherwise, stick with the default one.\n",
    "\n",
    "Note that when changing the batch size and sequence length the model will need to be re-compiled the first time it is run with the changed parameters.\n",
    "\n",
    "You can specify a checkpoint to be used for validation. If `None` is provided, the latest weights from the fine-tuning session will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify maximum number of tokens that can be generated.\n",
    "eval_config.inference.output_length = 5\n",
    "# Reducing sequence length\n",
    "max_len = reduce(lambda l, e: max(l, sum(e[\"attention_mask\"])), eval_dataset, 0)\n",
    "print(f\"Maximum length in mnli-mismatched dataset: {max_len}\")\n",
    "eval_config.model.sequence_length = max_len\n",
    "print(f\"Setting sequence length to {eval_config.model.sequence_length}\")\n",
    "# Increase batch size\n",
    "eval_config.execution.micro_batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can specify a directory containing a previous checkpoint\n",
    "# None means the latest weights are used\n",
    "eval_config.checkpoint.load = None  # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_config.dumps_yaml())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define a validation metric. We will use `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function to convert our generated labels to integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_mnli_predictions(generated_sentences):\n",
    "    labels_to_ids = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2, \"unknown\": -1}\n",
    "    predictions = []\n",
    "    for s in generated_sentences:\n",
    "        answer = mnli_data.extract_class_label(s)\n",
    "        predictions.append(labels_to_ids[answer])\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Trainer instance\n",
    "Once a config is specified, we are ready to create the training session with the help of the \n",
    "`T5Trainer` class.\n",
    "You can provide the following arguments:\n",
    "\n",
    "- `config`: the training configuration.\n",
    "- `pretrained`: the Hugging Face pre-trained model, used to initialise the weights.\n",
    "- `dataset`: the training dataset.\n",
    "- `eval_dataset`: the validation dataset.\n",
    "- `eval_config`: the inference configuration, to be used in validation.\n",
    "- `tokenizer`: the tokenizer, needed during validation.\n",
    "- `metric`: the metric for validation. A Hugging Face metric from the `evaluate` module. We use the `accuracy` metric.\n",
    "- `process_answers_func`: a function to convert the generated answers to the format required by the metric and the labels. For example we need to convert textual categories `[entailment, contradiction, neutral]` to indices.\n",
    "\n",
    "These arguments can also be provided later on when calling `trainer.train(...)` or `trainer.evaluate(...)`.\n",
    "\n",
    "If you want to run fine-tuning, the pre-trained model should be `google/flan-t5-xxl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.trainer import T5Trainer\n",
    "from transformers.models.t5.modeling_t5 import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will probably take a few minutes to load, due to the size of the model\n",
    "pretrained = T5ForConditionalGeneration.from_pretrained(f\"google/flan-t5-{model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = T5Trainer(\n",
    "    config,\n",
    "    pretrained,\n",
    "    dataset,\n",
    "    eval_dataset,\n",
    "    eval_config,\n",
    "    tokenizer,\n",
    "    accuracy_metric,\n",
    "    postprocess_mnli_predictions,\n",
    "    args,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run fine-tuning\n",
    "We can now run training for the number of steps you set in the config.\n",
    "Checkpoints will be saved in the folder you specified in `config.checkpoint.save`, with the periodicity set in `config.checkpoint.steps`.\n",
    "\n",
    "It will take around 15 minutes to compile the training model The first time you run `trainer.train()`.\n",
    "\n",
    "Training takes around 10 minutes to start because the pre-trained weights need to be downloaded to the IPUs.\n",
    "After that, each step takes around 36 seconds (with no data parallelism).\n",
    "This does not include time for checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run validation\n",
    "Finally, we validate our model on the [`validation_mismatched`](https://huggingface.co/datasets/glue/viewer/mnli_mismatched/test) split of the MNLI dataset.\n",
    "\n",
    "Generative inference is performed token-by-token using a greedy heuristic: the next token is chosen based on the highest logits.\n",
    "\n",
    "We run token-by-token inference in batches to generate labels for multiple examples at once.\n",
    "We then compute the accuracy by comparing the model answers with the true labels.\n",
    "\n",
    "The resulting model should achieve an accuracy of about 87% when fine-tuned for 500 steps.\n",
    "\n",
    "```\n",
    "Total number of examples                 9832\n",
    "Number with badly formed result          0\n",
    "Number with incorrect result             1253\n",
    "Number with correct result               8579 [87.3%]\n",
    " ```\n",
    "\n",
    "The first time you run `trainer.evaluate()` it takes around 6 minutes to compile the inference model.\n",
    "\n",
    "Running validation on the whole dataset takes around 18 minutes.\n",
    "\n",
    "Note that:\n",
    "- If you specify `trainer.evaluate(use_pretrained=True)`, we will use the weights from `pretrained`.\n",
    "- If you set `eval_config.checkpoint.load` to the path of a specific checkpoint, the weights from that checkpoint will be used.\n",
    "- If a training session exists we will use the latest weights from the training model.\n",
    "- If none of the above is true, we will use the weights from the latest checkpoint, if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Hugging Face checkpoint\n",
    "You can save the trained weights so that they can be uploaded to Hugging Face and used with Hugging Face's PyTorch model, on any hardware. For guidance on how to upload the saved model to the Model Hub, check the [Hugging Face documentation](https://huggingface.co/docs/hub/models-uploading).\n",
    "\n",
    "You can specify a checkpoint path if you want to convert a specific checkpoint instead of the latest fine-tuning weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_checkpoint_path = os.path.join(checkpoint_dir, \"hf_checkpoint\")\n",
    "ckpt_path = None  # os.path.join(checkpoint_dir, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned = trainer.save_hf_checkpoint(hf_checkpoint_path, ckpt_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model with Hugging Face pipeline\n",
    "The same model can later be used with the standard Hugging Face pipeline on any hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n",
    "hf_model = T5ForConditionalGeneration.from_pretrained(hf_checkpoint_path)\n",
    "generator = pipeline(\"text2text-generation\", model=hf_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"mnli hypothesis: Your contributions were of no help with our students' education. \"\n",
    "    \"premise: Your contribution helped make it possible for us to provide our students with a quality education.\"\n",
    ")\n",
    "\n",
    "out = generator(prompt)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook has demonstrated how easy it is to perform fine-tuning on Flan-T5 on the Graphcore IPU for a textual entailment task. While not as powerful as larger models for free text-generation, medium-size encoder-decoder models like Flan-T5 can still be successfully fine-tuned to handle a range of NLP downstream tasks such as question answering, sentiment analysis, and named entity recognition. In fact, for these kind of tasks you don't need models with 175B parameters like GPT-3. Flan-T5 XXL with 11B parameters has very good language understanding and is suitable for most language tasks. Larger models only give a small improvement in language understanding, but they do add more world knowledge. They therefore perform better at free text generation as might be used in an AI Assistant or chatbot.\n",
    "\n",
    "In this example we performed fine-tuning on Flan-T5 for textual entailment on the GLUE MNLI dataset.\n",
    "\n",
    "You can easily adapt this example to do your custom fine-tuning on several downstream tasks, such as question answering, named entity recognition, sentiment analysis, and text classification in general, simply by preparing your data accordingly.\n",
    "\n",
    "Overall, this notebook showcases the potential for Flan-T5 to be used effectively and efficiently for fine-tuning.\n",
    "\n",
    "If you'd like to use Flan-T5 XL and XXL to perform inference, check out the notebook in this directory [Flan-T5-generative-inference.ipynb](Flan-T5-generative-inference.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
