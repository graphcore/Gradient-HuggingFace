{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb7a2f86",
   "metadata": {},
   "source": [
    "# From MLOOps to MLOps\n",
    "\n",
    "Multi-generative demo which shows a (really bad) comic book generator from voice.\n",
    "\n",
    "This lifts most content from other notebooks:\n",
    "\n",
    "- `whisper/whisper-example.ipynb` for the automatic speech recognition\n",
    "- `natural-language-processing/FLAN-T5-generative-inference.ipynb` for the Text-to-text pipeline\n",
    "- And the stable diffusion notebooks\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56a3e2a5",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4be57731",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77c2229d-d6f5-4776-841a-7cf328050d30",
   "metadata": {},
   "source": [
    "IPU Whisper runs faster with the latest features available in SDK > 3.3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80006b6e-94d1-4954-8ad9-c9beb281473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "executable_cache_dir = \"/storage/mloops-demo/executable-caches\"\n",
    "\n",
    "%env POPTORCH_CACHE_DIR=/storage/mloops-demo/executable-caches\n",
    "%env POPLAR_EXECUTABLE_CACHE_DIR=/storage/mloops-demo/executable-caches\n",
    "%env HUGGINGFACE_HUB_CACHE=/storage/mloops-demo/huggingface_caches\n",
    "%env TRANSFORMERS_CACHE=/storage/mloops-demo/huggingface_caches/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6d46f-fe49-4701-82a7-0ab9c52e05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8a58c-73dc-4b7c-8f58-e843cb6f32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "sdk_version = !popc --version\n",
    "if sdk_version and (version := re.search(r'\\d+\\.\\d+\\.\\d+', sdk_version[0]).group()) >= '3.3':\n",
    "    print(f\"SDK check passed.\")\n",
    "    enable_sdk_features=True\n",
    "else:\n",
    "    warnings.warn(\"SDK versions lower than 3.3 do not support all the functionality in this notebook so performance will be reduced. We recommend you relaunch the Paperspace Notebook with the Pytorch SDK 3.3 image. You can use https://hub.docker.com/r/graphcore/pytorch-early-access\", \n",
    "                  category=Warning, stacklevel=2)\n",
    "    enable_sdk_features=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a87a013",
   "metadata": {},
   "source": [
    "If the above cell did not pass the SDK check, you can open a runtime with our SDK 3.3.0-EA enabled by clicking the Run on Gradient button below.\n",
    "\n",
    "[![Run on Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://ipu.dev/kC8VBy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "070d9b99",
   "metadata": {},
   "source": [
    "Install the dependencies the notebook needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde99b10-e2d2-4787-877f-fb120e327ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optimum from source \n",
    "%pip install \"optimum-graphcore>=0.6, <0.7\"\n",
    "%pip install soundfile==0.12.1 librosa==0.10.0.post2 tokenizers==0.12.1 gradio\n",
    "%pip install matplotlib fastapi\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08888a86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Automatic speech recognition with Whisper\n",
    "\n",
    "We start by importing the required modules, some of which are needed to configure the IPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6efd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import IPython\n",
    "import random\n",
    "\n",
    "# IPU-specific imports\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from optimum.graphcore.models.whisper import WhisperProcessorTorch\n",
    "\n",
    "# HF-related imports\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "734d8d54",
   "metadata": {},
   "source": [
    "The Whisper model is available on Hugging Face in several sizes, from `whisper-tiny` with 39M parameters to `whisper-large` with 1550M parameters.\n",
    "\n",
    "We download `whisper-tiny` which we will run using two IPUs.\n",
    "The [Whisper architecture](https://openai.com/research/whisper) is an encoder-decoder Transformer, with the audio split into 30-second chunks.\n",
    "For simplicity one IPU is used for the encoder part of the graph and another for the decoder part.\n",
    "The `IPUConfig` object helps to configure the model to be pipelined across the IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d72f3-cbd6-462f-9741-1726d412c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = \"openai/whisper-tiny.en\"\n",
    "\n",
    "# Instantiate processor and model\n",
    "processor = WhisperProcessorTorch.from_pretrained(model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_spec)\n",
    "\n",
    "# Adapt whisper-tiny to run on the IPU\n",
    "ipu_config = IPUConfig(ipus_per_replica=2)\n",
    "pipelined_model = to_pipelined(model, ipu_config)\n",
    "pipelined_model = pipelined_model.parallelize(\n",
    "    for_generation=True, \n",
    "    use_cache=True, \n",
    "    batch_size=1, \n",
    "    max_length=250,\n",
    "    on_device_generation_steps=16, \n",
    "    use_encoder_output_buffer=enable_sdk_features\n",
    ").half()\n",
    "\n",
    "def whisper_transcribe(data, rate):\n",
    "    input_features = processor(data, return_tensors=\"pt\", sampling_rate=rate).input_features.half()\n",
    "\n",
    "    # This triggers a compilation, unless a precompiled model is available.\n",
    "    sample_output = pipelined_model.generate(\n",
    "        input_features,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        max_length=448, \n",
    "        min_length=3)\n",
    "    transcription = processor.batch_decode(sample_output, skip_special_tokens=True)[0]\n",
    "    return transcription"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e99620b",
   "metadata": {},
   "source": [
    "Now we can load the dataset and process an example audio file.\n",
    "If precompiled models are not available, then the first run of the model triggers two graph compilations.\n",
    "This means that our first test transcription could take a minute or two to run, but subsequent runs will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab692b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and read an example sound file\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "test_sample = ds[2]\n",
    "sample_rate = test_sample['audio']['sampling_rate']\n",
    "\n",
    "\n",
    "test_transcription = whisper_transcribe(test_sample[\"audio\"][\"array\"], sample_rate)\n",
    "test_transcription"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa59411d",
   "metadata": {},
   "source": [
    "In the next cell, we compare the expected text from the dataset with the transcribed result from the model.\n",
    "There will typically be some small differences, but even `whisper-tiny` does a great job! It even adds punctuation.\n",
    "\n",
    "You can listen to the audio and compare the model result yourself using the controls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17947b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected: {test_sample['text']}\\n\")\n",
    "print(f\"Transcribed: {test_transcription}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "217f7821-1ddb-425e-995e-a9f084c7ff0b",
   "metadata": {},
   "source": [
    "The model only needs to be compiled once. Subsequent inferences will be much faster.\n",
    "In the cell below, we repeat the exercise but with a random example from the dataset.\n",
    "\n",
    "You might like to re-run this next cell multiple times to get different comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9ca3-a932-4e66-97c7-8ffe98d00bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, ds.num_rows - 1)\n",
    "data = ds[idx][\"audio\"][\"array\"]\n",
    "\n",
    "print(f\"Example #{idx}\\n\")\n",
    "print(f\"Expected: {ds[idx]['text']}\\n\")\n",
    "print(f\"Transcribed: {whisper_transcribe(data, sample_rate)}\")\n",
    "\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9692ea0b-c496-4b04-87c4-bfa8bb5e717c",
   "metadata": {},
   "source": [
    "# Running Flan-T5\n",
    "\n",
    "In this demo we use a Generative text model to refine our prompts for the Stable diffusion.\n",
    "\n",
    "In December 2022 Google published [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) in which they perform extensive fine-tuning for a broad collection of tasks across a variety of models (PaLM, T5, U-PaLM). Part of this publication was the release of Flan-T5 checkpoints, \"which achieve strong few-shot performance\" with relatively modest parameter counts \"even compared to much larger models\" like the largest members of the GPT family."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0cd9da7-8456-4eeb-bb36-7e9cf2047950",
   "metadata": {},
   "source": [
    "The cell below uses the Flan-T5 checkpoints from the Hugging Face Hub and uses the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b49ce2-f18a-44a7-951c-0965703b3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_t5_ipus=4\n",
    "\n",
    "from optimum.graphcore import pipeline, IPUConfig\n",
    "\n",
    "size = {4: \"large\", 16: \"xl\"}\n",
    "flan_t5 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=f\"google/flan-t5-{size[num_t5_ipus]}\",\n",
    "    ipu_config=IPUConfig.from_pretrained(\n",
    "        f\"Graphcore/t5-{size[num_t5_ipus]}-ipu\", executable_cache_dir=executable_cache_dir\n",
    "    ),\n",
    "    max_input_length=896,\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"Solve the following equation for x: x^2 - 9 = 0\",\n",
    "    \"At what temperature does nitrogen freeze?\",\n",
    "    \"In order to reduce symptoms of asthma such as tightness in the chest, wheezing, and difficulty breathing, what do you recommend?\",\n",
    "    \"Which country is home to the tallest mountain in the world?\"\n",
    "]\n",
    "for out in flan_t5(questions):\n",
    "    print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb82567c-38c8-490b-a29e-7d5026a1f469",
   "metadata": {},
   "source": [
    "The examples below are taken from the `natural-language-processing/FLAN-T5-Generative-inference` notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0164652-412d-435b-ab87-b12ae2535564",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = (\n",
    "    \"Review: It gets too hot, the battery only can last 4 hours. Sentiment: Negative\\n\"\n",
    "    \"Review: Nice looking phone. Sentiment: Positive\\n\"\n",
    "    \"Review: Sometimes it freezes and you have to close all the open pages and then reopen where you were. Sentiment: Negative\\n\"\n",
    "    \"Review: Wasn't that impressed, went back to my old phone. Sentiment:\"\n",
    ")\n",
    "advanced_ner = \"\"\"Microsoft Corporation is a company that makes computer software and video games. Bill Gates and Paul Allen founded the company in 1975\n",
    "[Company]: Microsoft, [Founded]: 1975, [Founders]: Bill Gates, Paul Allen\n",
    "\n",
    "Amazon.com, Inc., known as Amazon , is an American online business and cloud computing company. It was founded on July 5, 1994 by Jeff Bezos\n",
    "[Company]: Amazon, [Founded]: 1994, [Founders]: Jeff Bezos\n",
    "\n",
    "Apple Inc. is a multinational company that makes personal computers, mobile devices, and software. Apple was started in 1976 by Steve Jobs and Steve Wozniak.\"\"\"\n",
    "\n",
    "context = 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24-10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.'\n",
    "question = \"Which NFL team represented the AFC at Super Bowl 50?\"\n",
    "contextual_q_a = f\"{context} {question}\"\n",
    "\n",
    "intent_classification = \"\"\"[Text]: I really need to get a gym membership, I'm exhausted.\n",
    "[Intent]: get gym membership\n",
    "\n",
    "[Text]: What do I need to make a carbonara?\n",
    "[Intent]: cook carbonara\n",
    "\n",
    "[Text]: I need all these documents sorted and filed by Monday.\n",
    "[Intent]:\"\"\"\n",
    "\n",
    "summarization=\"\"\"\n",
    "Document: Firstsource Solutions said new staff will be based at its Cardiff Bay site which already employs about 800 people.\n",
    "The 300 new jobs include sales and customer service roles working in both inbound and outbound departments.\n",
    "The company's sales vice president Kathryn Chivers said: \"Firstsource Solutions is delighted to be able to continue to bring new employment to Cardiff.\"\n",
    "Summary: Hundreds of new jobs have been announced for a Cardiff call centre.\n",
    "\n",
    "Document: The visitors raced into a three-goal first-half lead at Hampden.\n",
    "Weatherson opened the scoring with an unstoppable 15th-minute free-kick, and he made it 2-0 in the 27th minute.\n",
    "Matt Flynn made it 3-0 six minutes later with a fine finish.\n",
    "Queen's pulled a consolation goal back in stoppage time through John Carter.\n",
    "Summary: Peter Weatherson netted a brace as Annan recorded only their second win in eight matches.\n",
    "\n",
    "Document: Officers searched properties in the Waterfront Park and Colonsay View areas of the city on Wednesday.\n",
    "Detectives said three firearms, ammunition and a five-figure sum of money were recovered.\n",
    "A 26-year-old man who was arrested and charged appeared at Edinburgh Sheriff Court on Thursday.\n",
    "Summary:\n",
    "\"\"\"\n",
    "text_classification_1 = \"\"\"A return ticket is better value than a single.\n",
    "topic: travel cost\n",
    "\n",
    "You can start from the basic stitches, and go from there.\n",
    "topic: learning knitting\n",
    "\n",
    "The desk which I bought yesterday is very big.\n",
    "topic: furniture size\n",
    "\n",
    "George Washington was president of the United States from 1789 to 1797.\n",
    "topic:\"\"\"\n",
    "text_classification_2 = \"\"\"FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models - it is an enhanced version of T5 that has been finetuned in a mixture of tasks.\n",
    "keywords: released, enhanced, finetuned\n",
    "\n",
    "The IPU, or Intelligence Processing Unit, is a highly flexible, easy-to-use parallel processor designed from the ground up for AI workloads.\n",
    "keywords: processor, AI\n",
    "\n",
    "Paperspace is the platform for AI developers. providing the speed and scale needed to take AI models from concept to production.\n",
    "keywords:\"\"\"\n",
    "\n",
    "flan_t5_examples = questions + [sentiment_analysis, advanced_ner, contextual_q_a, summarization, intent_classification, text_classification_1, text_classification_2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e213a76-acee-458c-8923-d7f74ee94f80",
   "metadata": {},
   "source": [
    "# Running stable diffusion\n",
    "\n",
    "We also instantiate stable diffusion for doing text to image generation.\n",
    "\n",
    "Stable diffusion is chosen to run on 8 IPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b496629-3209-489b-a1bc-e4dc74cc98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_stable_diffusion_ipus = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62d3bea3",
   "metadata": {},
   "source": [
    "As was the case for the other models, stable diffusion is loaded onto IPUs in a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a6fea-094d-4daf-9502-74d73aa04159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "from optimum.graphcore.diffusers import get_default_ipu_configs, INFERENCE_ENGINES_TO_MODEL_NAMES, IPUStableDiffusionPipeline\n",
    "\n",
    "\n",
    "engine = \"stable-diffusion-v1-5\"  # maps to \"runwayml/stable-diffusion-v1-5\"\n",
    "model_name = INFERENCE_ENGINES_TO_MODEL_NAMES[engine]\n",
    "image_width = os.getenv(\"STABLE_DIFFUSION_TXT2IMG_DEFAULT_WIDTH\", default=512)\n",
    "image_height = os.getenv(\"STABLE_DIFFUSION_TXT2IMG_DEFAULT_HEIGHT\", default=512)\n",
    "\n",
    "unet_ipu_config, text_encoder_ipu_config, vae_ipu_config, safety_checker_ipu_config = \\\n",
    "get_default_ipu_configs(\n",
    "    engine=engine, width=image_width, height=image_height, n_ipu=number_of_stable_diffusion_ipus, \n",
    "    executable_cache_dir=executable_cache_dir \n",
    ")\n",
    "pipe = IPUStableDiffusionPipeline.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"fp16\", \n",
    "    torch_dtype=torch.float16,\n",
    "    requires_safety_checker=False,\n",
    "    unet_ipu_config=unet_ipu_config,\n",
    "    text_encoder_ipu_config=text_encoder_ipu_config,\n",
    "    vae_ipu_config=vae_ipu_config,\n",
    "    safety_checker_ipu_config=safety_checker_ipu_config\n",
    ")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "309e54ae",
   "metadata": {},
   "source": [
    "The model is then compiled by running one inference loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e0961-7220-45ff-9117-65d768c1f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"apple\", height=image_height, width=image_width, guidance_scale=7.5);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0fddc2-a1bb-4fb1-a3f5-9233a45f50dc",
   "metadata": {},
   "source": [
    "# Assembling models in a single application\n",
    "\n",
    "In this section we will assembling the models into a simple Fast API web site which will have several of our apps.\n",
    "\n",
    "Fist we can check that our models are ready to run, we'll see below which models are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab83f94-3d63-48a7-b4f5-420002c311ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gc-monitor --no-card-info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56c2752b-6691-4906-b6d7-981fded0e52f",
   "metadata": {},
   "source": [
    "Test the audio model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27325184-fa97-42b3-9d2d-b210220372cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 58\n",
    "\n",
    "transcription = whisper_transcribe(data, sample_rate)\n",
    "print(f\"Example #{idx}\\n\")\n",
    "print(f\"Expected: {ds[idx]['text']}\\n\")\n",
    "print(f\"Transcribed: {transcription}\")\n",
    "\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd24149-e053-4e2d-a2fe-f4a9af0e13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little helper function for turning Librispeech IDs into audio samples\n",
    "\n",
    "def sample_audio(audio_prompts):\n",
    "    f, m, *l = audio_prompts\n",
    "    audio_prompts = f, min(m, len(ds)), *l\n",
    "    return np.concatenate([ds[idx][\"audio\"][\"array\"] for idx in range(*audio_prompts)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba334d6f-72b4-4645-a140-733596298d09",
   "metadata": {},
   "source": [
    "Test linking audio and image models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdf64a-06c6-400a-8b7a-47dc0a167b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 58\n",
    "data = ds[idx][\"audio\"][\"array\"]\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)\n",
    "transcription = whisper_transcribe(data, sample_rate)\n",
    "print(transcription)\n",
    "out = pipe(transcription, height=image_height, width=image_width, guidance_scale=7.5)\n",
    "out.images[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7413e62-12e0-49b7-93fb-de6a32a3724d",
   "metadata": {},
   "source": [
    "Now the output from the ASR model does not make a good input to stable diffusion prompts.\n",
    "\n",
    "To do that, we use FLAN-T5 to reformat our sentences, into stable diffusion prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d13bf2-d701-4d6c-b2e4-b8a06ee66f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LLM_prompt = \"\"\"\n",
    "[Long text]: {previous_story}\n",
    "[source] {previous_sentence} [description]:\n",
    "\"\"\"\n",
    "\n",
    "LLM_continue = \"\"\"\n",
    "{previous_story}\n",
    "Continue the story without repetition:\n",
    "\"\"\"\n",
    "\n",
    "def LLM_to_sd_prompts(generated_text, prompt_for_sd=LLM_prompt, prompt_to_continue=LLM_continue, story_steps=3, llm_replace=False):\n",
    "    print(\"Continuing story\")\n",
    "    for i in range(story_steps):\n",
    "        arg = dict(\n",
    "            previous_story = \" \".join(generated_text),\n",
    "            previous_sentence = generated_text[-1],\n",
    "        )\n",
    "        prompt = prompt_to_continue.format(**arg)\n",
    "        out_text = flan_t5(prompt)\n",
    "        generated_text.append(out_text[0]['generated_text'])\n",
    "    \n",
    "    print(\"Transformming Stable diffusion prompts\")\n",
    "    transformed_text = []\n",
    "    if llm_replace:\n",
    "        for i, sentence in enumerate(generated_text):\n",
    "            arg = dict(\n",
    "                previous_story = \" \".join(generated_text[:i]),\n",
    "                previous_sentence = generated_text[i],\n",
    "            )\n",
    "            prompt = prompt_for_sd.format(**arg)\n",
    "            out_text = flan_t5(prompt)\n",
    "            transformed_text.append(out_text[0]['generated_text'])\n",
    "    else:\n",
    "        transformed_text.extend(generated_text)\n",
    "\n",
    "    return transformed_text\n",
    "\n",
    "LLM_to_sd_prompts([transcription], LLM_prompt, LLM_continue, llm_replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270995f-dc40-4640-82db-fb3ddc0dc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "audio_prompts = (10, 30)\n",
    "# style_prompt = \"japanese, manga, high resolution, dynamic.\"\n",
    "style_prompt = \"modern art, smooth vibes\"\n",
    "\n",
    "\n",
    "def generate_comic(audio, style_prompt, prompt_for_sd=LLM_prompt, prompt_to_continue=LLM_continue, max_images=None, sample_rate=sample_rate, generate_image=True, llm_replace=False):\n",
    "    generated_text = []\n",
    "    print(\"Transcribing audio\")\n",
    "    \n",
    "    transcription = \"\"\n",
    "    max_model_samples = sample_rate * 30\n",
    "    for i in range(0, len(audio), max_model_samples):\n",
    "        print(f\"  Audio was {len(audio) // sample_rate} seconds long, processing part: {i/max_model_samples+1} in chunks of 30s\")\n",
    "        transcription += whisper_transcribe(audio[i:(i+1)*max_model_samples], sample_rate)\n",
    "\n",
    "    generated_text = [f\"{t}. \" for t in transcription.split(\".\")]\n",
    "    generated_text = LLM_to_sd_prompts(generated_text, prompt_for_sd=prompt_for_sd, prompt_to_continue=prompt_to_continue, story_steps=3, llm_replace=llm_replace)\n",
    "    images = []\n",
    "    if not generate_image:\n",
    "        return transcription, generated_text, images, audio\n",
    "    print(\"generating images\")\n",
    "    nstep = 0\n",
    "    for prompt in generated_text:\n",
    "        out = pipe(prompt + style_prompt, height=image_height, width=image_width, guidance_scale=7.5)\n",
    "        images.append(out.images[0])\n",
    "        nstep +=1\n",
    "        if not (max_images is None) and nstep >= max_images:\n",
    "            break\n",
    "    print(generated_text)\n",
    "    return transcription, generated_text, images, audio\n",
    "\n",
    "transcription, generated_text, images, data = generate_comic(sample_audio(audio_prompts), style_prompt, max_images=4)\n",
    "print(\"rendering audio\")\n",
    "print(transcription)\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd085d21-9093-4ad4-9e38-3145a3f0a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "def comic_book_plotter(generated_text, images, style_prompt, image_per_page=6, line_break=40, fig_size=(8,15)):\n",
    "    name = style_prompt.replace(\" \",\"_\").replace(\".\",\"_\").replace(\".\",\"_\").strip(\"_\")\n",
    "    comic_hash = hash(\"\\n\".join(generated_text)) + sum(hash(image.tobytes()) for image in images)\n",
    "    figs = []\n",
    "    paths = []\n",
    "    for page_num, id_start in enumerate(range(0,len(generated_text), image_per_page)):\n",
    "        comic_text = generated_text[id_start:id_start+image_per_page]\n",
    "        comic_images = images[id_start:id_start+image_per_page]\n",
    "        if not comic_images:\n",
    "            break\n",
    "        fig, axs = plt.subplots(image_per_page//2, 2)\n",
    "        figs.append(fig)\n",
    "        fig.set_size_inches(*fig_size)\n",
    "        \n",
    "        for image, prompt, ax in zip(comic_images, comic_text,axs.flatten()):\n",
    "            ax.imshow(image)\n",
    "            breaks = [0] + [prompt.find(\" \", i) for i in range(line_break, len(prompt), line_break)] + [-1]\n",
    "            formatted_prompt = \"\\n\".join(prompt[i:j] for i, j in zip(breaks[:-1], breaks[1:]))\n",
    "            ax.set_title(f\"{formatted_prompt}\")\n",
    "            ax.axis(\"off\")\n",
    "        fig.suptitle(f\"Style prompt: '{style_prompt}' page {page_num+1}\", y=1.0)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        pathlib.Path(\"/storage/comics/\").mkdir(exist_ok=True)\n",
    "        image_path = f\"/storage/comics/whisper_to_image_{name}_{comic_hash}-page-{page_num+1}.png\"\n",
    "        paths.append(image_path)\n",
    "        fig.savefig(image_path, dpi=150)\n",
    " \n",
    "    return figs, paths\n",
    "\n",
    "fig, image_path = comic_book_plotter(generated_text, images, style_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48bb15-b10c-487f-8204-97ce10815cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import librosa\n",
    "\n",
    "def mic_to_text(rate_and_audio, uploaded_sound):\n",
    "    if rate_and_audio is None:\n",
    "        rate_and_audio = uploaded_sound\n",
    "    source_sample_rate, source_audio= rate_and_audio\n",
    "    model_sr=16000\n",
    "    audio = librosa.resample(y=source_audio.astype(float)/np.iinfo(source_audio.dtype).max, orig_sr=source_sample_rate, target_sr=model_sr)\n",
    "    transcription = \"\"\n",
    "    max_model_samples = model_sr * 30\n",
    "    for i in range(0, len(audio), max_model_samples):\n",
    "        print(f\"  Audio was {len(audio) // sample_rate} seconds long, truncating at 30s\")\n",
    "        transcription += whisper_transcribe(audio[i:(i+1)*max_model_samples], sample_rate)\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def text_to_text(prompt):\n",
    "    return flan_t5(prompt)[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "\n",
    "def text_to_image(prompt, n_repeats=4):\n",
    "    return [pipe(prompt, height=image_height, width=image_width, guidance_scale=7.5).images[0] for _ in range(n_repeats)]\n",
    "\n",
    "\n",
    "\n",
    "def transcribe_to_comic(rate_and_audio, style_prompt, max_images, transform_sentence_to_prompts, llm_to_sd_prompt, uploaded_sound):\n",
    "    if rate_and_audio is None:\n",
    "        rate_and_audio = uploaded_sound\n",
    "    sample_rate, audio= rate_and_audio\n",
    "    target_sr=16000\n",
    "    resample_audio = librosa.resample(y=audio.astype(float)/np.iinfo(audio.dtype).max, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    transcription, generated_text, images, data = generate_comic(\n",
    "        resample_audio, style_prompt, sample_rate=target_sr, max_images=max_images, prompt_for_sd=llm_to_sd_prompt, llm_replace=transform_sentence_to_prompts\n",
    "    )\n",
    "    figs, paths = comic_book_plotter(generated_text, images, style_prompt, image_per_page=2, line_break=40, fig_size=(8,8))\n",
    "    return transcription, paths, \"\\n\".join(generated_text)\n",
    "\n",
    "import soundfile\n",
    "\n",
    "def to_sample_audio_file(libri_id):\n",
    "    audio = sample_audio(libri_id)\n",
    "    file = f\"librispeech_validation_{libri_id[0]}-{libri_id[1]}.wav\"\n",
    "    soundfile.write(file, audio, 16000)\n",
    "    return file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afff350-6843-4916-8068-14c9b55e62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_length = 15\n",
    "audio_examples= [\n",
    "    to_sample_audio_file((i, i+audio_length)) for i in range(0, len(ds), audio_length)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef5d84-92f7-4e86-b0eb-f389df6e6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "comic_book_app = gr.Interface(\n",
    "    fn=transcribe_to_comic,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\", type=\"numpy\"),\n",
    "        \"text\",\n",
    "        gr.Slider(value=10),\n",
    "        gr.Checkbox(value=False),\n",
    "        gr.TextArea(value=LLM_prompt),\n",
    "        gr.Audio(source=\"upload\", type=\"numpy\"),\n",
    "    ], \n",
    "    outputs=[\"text\", gr.Gallery(min_width=800, preview=False), gr.TextArea()],\n",
    "    examples=[\n",
    "        list(e) for e in zip(audio_examples,\n",
    "        [\n",
    "            \"Aquarel, calm.\",\n",
    "            \"Dali, surrealistic, dreamy.\",\n",
    "            \"Manga, high resolution, dynamic.\",\n",
    "            \"Turner, oil painting.\",\n",
    "        ]* len(audio_examples),\n",
    "        [None] * len(audio_examples),\n",
    "        [None] * len(audio_examples),\n",
    "        [None] * len(audio_examples),\n",
    "     )\n",
    "    ]\n",
    ").queue()\n",
    "\n",
    "asr_app = gr.Interface(\n",
    "    fn=mic_to_text,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\", type=\"numpy\"),\n",
    "        gr.Audio(source=\"upload\", type=\"numpy\"),\n",
    "    ], \n",
    "    outputs=\"text\",\n",
    "    examples=[[e, None] for e in audio_examples],\n",
    ").queue()\n",
    "\n",
    "text_app = gr.Interface(\n",
    "    fn=text_to_text,\n",
    "    inputs=\"text\", \n",
    "    outputs=\"text\",\n",
    "    examples=flan_t5_examples,\n",
    ").queue()\n",
    "\n",
    "image_app = gr.Interface(\n",
    "    fn=text_to_image,\n",
    "    inputs=\"text\",\n",
    "    outputs=gr.Gallery(preview=False),\n",
    "    examples=[\n",
    "        \"A digital illustration of a steampunk library with clockwork machines, 4k, detailed, trending in artstation, fantasy vivid colors\",\n",
    "        \"A digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors\",\n",
    "        \"A digital Illustration of the Babel tower, 4k, detailed, trending in artstation, fantasy vivid colors\",\n",
    "        \"A medieval town with disco lights and a fountain, by Josef Thoma, matte painting trending on artstation HQ, concept art\",\n",
    "        \"Editorial Style Photo, (Low Angle|Eye Level), ${THING YOU WANT}, Task Lighting, {MATERIALS}, {STYLE ADJECTIVES}, Symmetric, 4k\",\n",
    "    ],\n",
    "    \n",
    ").queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54245243-d028-4c52-bd09-0f9ed48d7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from fastapi.responses import HTMLResponse\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "def read_main():\n",
    "    return \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Welcome to the MLOops demo</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>Welcome to the MLOops demo!</h1>\n",
    "\n",
    "<p>\n",
    "This is a collection of Gradio apps demonstrating:\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"/voice\">Voice transcription with whisper</a></li>\n",
    "<li><a href=\"/text\">Text to text generation with T5-large</a></li>\n",
    "<li><a href=\"/image\">Text to image Stable diffusion</a></li>\n",
    "</ul>\n",
    "\n",
    "And all those models are assembled into...\n",
    "<ul>\n",
    "<li><a href=\"/comic\">A very bad comic book generator</a></li>\n",
    "</ul>\n",
    "\n",
    "</p>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "\n",
    "app = gr.mount_gradio_app(app, comic_book_app, path=\"/comic\")\n",
    "app = gr.mount_gradio_app(app, text_app, path=\"/text\")\n",
    "app = gr.mount_gradio_app(app, image_app, path=\"/image\")\n",
    "app = gr.mount_gradio_app(app, asr_app, path=\"/voice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44dc2bc-040f-4e37-9f10-721ecdb6fe5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# Needed for coroutine's in Notebooks\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "print(\"got event loop\")\n",
    "\n",
    "running_app = loop.run_until_complete(\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=int(os.getenv('APP_PORT', 7860)))\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fbf73c9-5449-441d-a59a-c4eb315d5489",
   "metadata": {},
   "source": [
    "# Try a model for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0fca5-70b4-4c64-936e-9b7756ad64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_prompt = pipeline('text-generation', model=\"Gustavosta/MagicPrompt-Stable-Diffusion\", ipu_config=\"Graphcore/gpt2-small-ipu\")\n",
    "magic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ac4e3-a374-4fa1-a0c3-a5534227905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_prompt(\"this is a file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c2bc5-d4ef-4684-93cb-626b61170330",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5.model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a8389-7557-482e-b273-4f935c0bd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_prompt.model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97cbed-3d67-4626-968a-cc7e3f3bafca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
