{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7a2f86",
   "metadata": {},
   "source": [
    "# Speech Transcription on IPUs using Whisper - Inference\n",
    "\n",
    "This notebook demonstrates speech transcription on the IPU using the [Whisper implementation in the Hugging Face Transformers library](https://huggingface.co/spaces/openai/whisper) alongside [Optimum Graphcore](https://github.com/huggingface/optimum-graphcore).\n",
    "\n",
    "Whisper is a versatile speech recognition model that can transcribe speech as well as perform multi-lingual translation and recognition tasks.\n",
    "It was trained on diverse datasets to give human-level speech recognition performance without the need for fine tuning. \n",
    "\n",
    "[ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) is the interface between the [ðŸ¤— Transformers library](https://huggingface.co/docs/transformers/index) and [Graphcore IPUs](https://www.graphcore.ai/products/ipu).\n",
    "It provides a set of tools enabling model parallelization and loading on IPUs, training and fine-tuning on all the tasks already supported by Transformers while being compatible with the Hugging Face Hub and every model available on it out of the box.\n",
    "\n",
    "> **Hardware requirements:** The Whisper models `whisper-tiny`, `whisper-base` and `whisper-small` can run two replicas on the smallest IPU-POD4 machine. The most capable model, `whisper-large`, will need to use either an IPU-POD16 or a Bow Pod16 machine. Please contact Graphcore if you'd like assistance running model sizes that don't work in this simple example notebook.\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3e2a5",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "The best way to run this demo is on Paperspace Gradient's cloud IPUs because everything is already set up for you.\n",
    "\n",
    "To run the demo using other IPU hardware, you need to have the Poplar SDK enabled. Refer to the [Getting Started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for details on how to enable the Poplar SDK. Also refer to the [Jupyter Quick Start guide](https://docs.graphcore.ai/projects/jupyter-notebook-quick-start/en/latest/index.html) for how to set up Jupyter to be able to run this notebook on a remote IPU machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be57731",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2229d-d6f5-4776-841a-7cf328050d30",
   "metadata": {},
   "source": [
    "IPU Whisper runs faster with the latest features available in SDK > 3.3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6d46f-fe49-4701-82a7-0ab9c52e05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c7f87-307e-4430-b052-5d5ec0febcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c8a58c-73dc-4b7c-8f58-e843cb6f32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "sdk_version = !popc --version\n",
    "if sdk_version and (version := re.search(r'\\d+\\.\\d+\\.\\d+', sdk_version[0]).group()) >= '3.3':\n",
    "    print(f\"SDK check passed.\")\n",
    "    enable_sdk_features=True\n",
    "else:\n",
    "    warnings.warn(\"SDK versions lower than 3.3 do not support all the functionality in this notebook so performance will be reduced. We recommend you relaunch the Paperspace Notebook with the Pytorch SDK 3.3 image. You can use https://hub.docker.com/r/graphcore/pytorch-early-access\", \n",
    "                  category=Warning, stacklevel=2)\n",
    "    enable_sdk_features=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87a013",
   "metadata": {},
   "source": [
    "If the above cell did not pass the SDK check, you can open a runtime with our SDK 3.3.0-EA enabled by clicking the Run on Gradient button below.\n",
    "\n",
    "[![Run on Gradient](https://assets.paperspace.io/img/gradient-badge.svg)](https://ipu.dev/kC8VBy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d9b99",
   "metadata": {},
   "source": [
    "Install the dependencies the notebook needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde99b10-e2d2-4787-877f-fb120e327ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optimum from source \n",
    "%pip install \"optimum-graphcore>=0.6, <0.7\"\n",
    "%pip install soundfile==0.12.1 librosa==0.10.0.post2 tokenizers==0.12.1 gradio\n",
    "%pip install matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08888a86",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Whisper on the IPU\n",
    "\n",
    "We start by importing the required modules, some of which are needed to configure the IPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6efd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import IPython\n",
    "import random\n",
    "\n",
    "# IPU-specific imports\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from optimum.graphcore.models.whisper import WhisperProcessorTorch\n",
    "\n",
    "# HF-related imports\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d8d54",
   "metadata": {},
   "source": [
    "The Whisper model is available on Hugging Face in several sizes, from `whisper-tiny` with 39M parameters to `whisper-large` with 1550M parameters.\n",
    "\n",
    "We download `whisper-tiny` which we will run using two IPUs.\n",
    "The [Whisper architecture](https://openai.com/research/whisper) is an encoder-decoder Transformer, with the audio split into 30-second chunks.\n",
    "For simplicity one IPU is used for the encoder part of the graph and another for the decoder part.\n",
    "The `IPUConfig` object helps to configure the model to be pipelined across the IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d72f3-cbd6-462f-9741-1726d412c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = \"openai/whisper-tiny.en\"\n",
    "\n",
    "# Instantiate processor and model\n",
    "processor = WhisperProcessorTorch.from_pretrained(model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_spec)\n",
    "\n",
    "# Adapt whisper-tiny to run on the IPU\n",
    "ipu_config = IPUConfig(ipus_per_replica=2)\n",
    "pipelined_model = to_pipelined(model, ipu_config)\n",
    "pipelined_model = pipelined_model.parallelize(\n",
    "    for_generation=True, \n",
    "    use_cache=True, \n",
    "    batch_size=1, \n",
    "    max_length=250,\n",
    "    on_device_generation_steps=16, \n",
    "    use_encoder_output_buffer=enable_sdk_features).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99620b",
   "metadata": {},
   "source": [
    "Now we can load the dataset and process an example audio file.\n",
    "If precompiled models are not available, then the first run of the model triggers two graph compilations.\n",
    "This means that our first test transcription could take a minute or two to run, but subsequent runs will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab692b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and read an example sound file\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "test_sample = ds[2]\n",
    "sample_rate = test_sample['audio']['sampling_rate']\n",
    "\n",
    "def whisper_transcribe(data, rate):\n",
    "    input_features = processor(data, return_tensors=\"pt\", sampling_rate=rate).input_features.half()\n",
    "\n",
    "    # This triggers a compilation, unless a precompiled model is available.\n",
    "    sample_output = pipelined_model.generate(\n",
    "        input_features,\n",
    "        use_cache=True,\n",
    "        do_sample=False,\n",
    "        max_length=448, \n",
    "        min_length=3)\n",
    "    transcription = processor.batch_decode(sample_output, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "test_transcription = whisper_transcribe(test_sample[\"audio\"][\"array\"], sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59411d",
   "metadata": {},
   "source": [
    "In the next cell, we compare the expected text from the dataset with the transcribed result from the model.\n",
    "There will typically be some small differences, but even `whisper-tiny` does a great job! It even adds punctuation.\n",
    "\n",
    "You can listen to the audio and compare the model result yourself using the controls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b760d-f518-4363-bc48-b46a05fa27ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17947b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected: {test_sample['text']}\\n\")\n",
    "print(f\"Transcribed: {test_transcription}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f7821-1ddb-425e-995e-a9f084c7ff0b",
   "metadata": {},
   "source": [
    "The model only needs to be compiled once. Subsequent inferences will be much faster.\n",
    "In the cell below, we repeat the exercise but with a random example from the dataset.\n",
    "\n",
    "You might like to re-run this next cell multiple times to get different comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9ca3-a932-4e66-97c7-8ffe98d00bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, ds.num_rows - 1)\n",
    "data = ds[idx][\"audio\"][\"array\"]\n",
    "\n",
    "print(f\"Example #{idx}\\n\")\n",
    "print(f\"Expected: {ds[idx]['text']}\\n\")\n",
    "print(f\"Transcribed: {whisper_transcribe(data, sample_rate)}\")\n",
    "\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9692ea0b-c496-4b04-87c4-bfa8bb5e717c",
   "metadata": {},
   "source": [
    "# Running Flan-T5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd9da7-8456-4eeb-bb36-7e9cf2047950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b49ce2-f18a-44a7-951c-0965703b3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_cache_dir=os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache/\")\n",
    "num_t5_ipus=4\n",
    "\n",
    "from optimum.graphcore import pipeline, IPUConfig\n",
    "\n",
    "size = {4: \"large\", 16: \"xl\"}\n",
    "flan_t5 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=f\"google/flan-t5-{size[num_t5_ipus]}\",\n",
    "    ipu_config=IPUConfig.from_pretrained(\n",
    "        f\"Graphcore/t5-{size[num_t5_ipus]}-ipu\", executable_cache_dir=executable_cache_dir\n",
    "    ),\n",
    "    max_input_length=896,\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"Solve the following equation for x: x^2 - 9 = 0\",\n",
    "    \"At what temperature does nitrogen freeze?\",\n",
    "    \"In order to reduce symptoms of asthma such as tightness in the chest, wheezing, and difficulty breathing, what do you recommend?\",\n",
    "    \"Which country is home to the tallest mountain in the world?\"\n",
    "]\n",
    "for out in flan_t5(questions):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e213a76-acee-458c-8923-d7f74ee94f80",
   "metadata": {},
   "source": [
    "# Running stable diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b496629-3209-489b-a1bc-e4dc74cc98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_stable_diffusion_ipus = 8\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/stablediffusion_to-image\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6564caf-ba06-4c1b-96b9-d31e9ee47eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "from optimum.graphcore.diffusers import get_default_ipu_configs, INFERENCE_ENGINES_TO_MODEL_NAMES, IPUStableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a6fea-094d-4daf-9502-74d73aa04159",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = \"stable-diffusion-v1-5\"  # maps to \"runwayml/stable-diffusion-v1-5\"\n",
    "model_name = INFERENCE_ENGINES_TO_MODEL_NAMES[engine]\n",
    "image_width = os.getenv(\"STABLE_DIFFUSION_TXT2IMG_DEFAULT_WIDTH\", default=512)\n",
    "image_height = os.getenv(\"STABLE_DIFFUSION_TXT2IMG_DEFAULT_HEIGHT\", default=512)\n",
    "\n",
    "unet_ipu_config, text_encoder_ipu_config, vae_ipu_config, safety_checker_ipu_config = \\\n",
    "get_default_ipu_configs(\n",
    "    engine=engine, width=image_width, height=image_height, n_ipu=number_of_stable_diffusion_ipus, \n",
    "    executable_cache_dir=executable_cache_dir \n",
    ")\n",
    "pipe = IPUStableDiffusionPipeline.from_pretrained(\n",
    "    model_name,\n",
    "    revision=\"fp16\", \n",
    "    torch_dtype=torch.float16,\n",
    "    requires_safety_checker=False,\n",
    "    unet_ipu_config=unet_ipu_config,\n",
    "    text_encoder_ipu_config=text_encoder_ipu_config,\n",
    "    vae_ipu_config=vae_ipu_config,\n",
    "    safety_checker_ipu_config=safety_checker_ipu_config\n",
    ")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e0961-7220-45ff-9117-65d768c1f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"apple\", height=image_height, width=image_width, guidance_scale=7.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fddc2-a1bb-4fb1-a3f5-9233a45f50dc",
   "metadata": {},
   "source": [
    "# The demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab83f94-3d63-48a7-b4f5-420002c311ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gc-monitor --no-card-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27325184-fa97-42b3-9d2d-b210220372cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 58\n",
    "\n",
    "transcription = whisper_transcribe(data, sample_rate)\n",
    "print(f\"Example #{idx}\\n\")\n",
    "print(f\"Expected: {ds[idx]['text']}\\n\")\n",
    "print(f\"Transcribed: {transcription}\")\n",
    "\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdf64a-06c6-400a-8b7a-47dc0a167b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 58\n",
    "data = ds[idx][\"audio\"][\"array\"]\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)\n",
    "transcription = whisper_transcribe(data, sample_rate)\n",
    "print(transcription)\n",
    "out = pipe(transcription, height=image_height, width=image_width, guidance_scale=7.5)\n",
    "out.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e2694-a198-4fc1-b7f7-3c3a52d867f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d13bf2-d701-4d6c-b2e4-b8a06ee66f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "audio_prompts = (10, 30)\n",
    "# style_prompt = \"japanese, manga, high resolution, dynamic.\"\n",
    "style_prompt = \"modern art, smooth vibes\"\n",
    "LLM_prompt = lambda previous_story: f\"\"\"\n",
    "[Long text]: {' '.join(previous_story)}\n",
    "[summary]:\n",
    "\"\"\"\n",
    "\n",
    "def sample_audio(audio_prompts):\n",
    "    return np.concatenate([ds[idx][\"audio\"][\"array\"] for idx in range(*audio_prompts)])\n",
    "\n",
    "def generate_comic(audio, style_prompt, LLM_prompt=LLM_prompt, sample_rate=sample_rate, generate_image=True):\n",
    "    generated_text = []\n",
    "    print(\"Transcribing audio\")\n",
    "    \n",
    "    if len(audio) > sample_rate * 30:\n",
    "        print(f\"  Audio was {len(audio) // sample_rate} seconds long, truncating at 30s\")\n",
    "        audio = audio[:sample_rate*30]\n",
    "    transcription = whisper_transcribe(audio, sample_rate)\n",
    "\n",
    "    generated_text = [f\"{t}.\" for t in transcription.split(\".\")]\n",
    "    print(\"Generating story\")\n",
    "    for i in range(5):\n",
    "        prompt = LLM_prompt(generated_text)\n",
    "        out_text = flan_t5(prompt)\n",
    "        generated_text.append(out_text[0]['generated_text'])\n",
    "    images = []\n",
    "    if not generate_image:\n",
    "        return transcription, generated_text, images, audio\n",
    "    print(\"generating images\")\n",
    "    for prompt in generated_text:\n",
    "        out = pipe(prompt + style_prompt, height=image_height, width=image_width, guidance_scale=7.5)\n",
    "        images.append(out.images[0])\n",
    "    print(generated_text)\n",
    "    return transcription, generated_text, images, audio\n",
    "\n",
    "transcription, generated_text, images, data = generate_comic(sample_audio(audio_prompts), style_prompt, LLM_prompt)\n",
    "print(\"rendering audio\")\n",
    "print(transcription)\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd085d21-9093-4ad4-9e38-3145a3f0a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pathlib\n",
    "\n",
    "def comic_book_plotter(generated_text, images, style_prompt, image_per_page=6, line_break=40, fig_size=(8,15)):\n",
    "    name = style_prompt.replace(\" \",\"_\").replace(\".\",\"_\").replace(\".\",\"_\").strip(\"_\")\n",
    "    comic_hash = hash(\"\\n\".join(generated_text)) + sum(hash(image.tobytes()) for image in images)\n",
    "    figs = []\n",
    "    paths = []\n",
    "    for page_num, id_start in enumerate(range(0,len(generated_text), image_per_page)):\n",
    "        comic_text = generated_text[id_start:id_start+image_per_page]\n",
    "        comic_images = images[id_start:id_start+image_per_page]\n",
    "        fig, axs = plt.subplots(image_per_page//2, 2)\n",
    "        figs.append(fig)\n",
    "        fig.set_size_inches(*fig_size)\n",
    "        for image, prompt, ax in zip(comic_images, comic_text,axs.flatten()):\n",
    "            ax.imshow(image)\n",
    "            breaks = [0] + [prompt.find(\" \", i) for i in range(line_break, len(prompt), line_break)] + [-1]\n",
    "            formatted_prompt = \"\\n\".join(prompt[i:j] for i, j in zip(breaks[:-1], breaks[1:]))\n",
    "            ax.set_title(f\"{formatted_prompt}\")\n",
    "            ax.axis(\"off\")\n",
    "        fig.suptitle(f\"Style prompt: '{style_prompt}' page {page_num+1}\", y=1.0)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        pathlib.Path(\"/storage/comics/\").mkdir(exist_ok=True)\n",
    "        image_path = f\"/storage/comics/whisper_to_image_{name}_{comic_hash}-page-{page_num+1}.png\"\n",
    "        paths.append(image_path)\n",
    "        fig.savefig(image_path, dpi=150)\n",
    " \n",
    "    return figs, paths\n",
    "\n",
    "fig, image_path = comic_book_plotter(generated_text, images, style_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48bb15-b10c-487f-8204-97ce10815cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import librosa\n",
    "def transcribe_to_comic(rate_and_audio, style_prompt):\n",
    "    audio_from_mic = rate_and_audio\n",
    "    sample_rate, audio= rate_and_audio\n",
    "    target_sr=16000\n",
    "    resample_audio = librosa.resample(y=audio.astype(float)/np.iinfo(audio.dtype).max, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    transcription, generated_text, images, data = generate_comic(resample_audio, style_prompt, sample_rate=target_sr)\n",
    "    figs, paths = comic_book_plotter(generated_text, images, style_prompt, image_per_page=2, line_break=40, fig_size=(8,8))\n",
    "    return transcription, paths\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe_to_comic,\n",
    "    inputs=[\n",
    "        gr.Audio(source=\"microphone\", type=\"numpy\"),\n",
    "        \"text\",\n",
    "    ], \n",
    "    outputs=[\"text\", gr.Gallery(min_width=800, preview=True)]\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0558d1ae-4781-4443-8121-04a92e12e3d2",
   "metadata": {},
   "source": [
    "# Creating a tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27735890-10f2-409d-b01f-abed96f4a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c11f8-5329-41a3-9f7a-e36591fb08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to start ngrok tunnel in a notebook environment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ngrok token\n",
    "import os\n",
    "# Insert your ngrok authentication token here:\n",
    "os.environ['NGROK_AUTHTOKEN'] = \"\"\n",
    "import ngrok\n",
    "\n",
    "# Needed for coroutine's in Notebooks\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "tunnel = loop.run_until_complete(ngrok.werkzeug_develop())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ddf86-bb7a-4140-98bd-a2a1e41aeda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngrok\n",
    "\n",
    "import os\n",
    "# Insert your ngrok authentication token here:\n",
    "os.environ['NGROK_AUTHTOKEN'] = \"\"\n",
    "public_url = ngrok.connect(port = '7872')\n",
    "public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbcabe6-8145-4c07-b9a8-c9e4ddc5c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ngrok free tier only one active tunne is allowed at one time. This is a problem is the notebook times out (or runs in the background, in a closed page) as ngrok will fail to create a new tunnel.\n",
    "# I aven't been able to find how to kill the old tunnels.  \n",
    "# ngrok.disconnect(\"https://2ba3-38-83-162-251.ngrok-free.app/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
