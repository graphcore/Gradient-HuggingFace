{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e89a65",
   "metadata": {},
   "source": [
    "# Fine-tune a wav2vec 2 checkpoint for Automatic Speech Recognition (ASR) on IPU\n",
    "\n",
    "This notebook will demonstrate how to fine-tune a pre-trained wav2vec 2.0 model with PyTorch on the Graphcore IPUs. We will use a \"wav2vec2-base\" model and fine-tune for a CTC downstream task using LibriSpeech.\n",
    "\n",
    "We will show how to use a wav2vec 2.0 model written in PyTorch from the ğŸ¤—`transformers` library from HuggingFace and paralellize it using the ğŸ¤—`optimum-graphcore` library.\n",
    "\n",
    "### Background\n",
    "\n",
    "ASR (Automatic Speech Recognition), the task of transcribing audio automatically, has historically required large amounts of labelled data. Additionally, these systems had predominantly used fixed feature extraction methods which do not learn from the raw signal, for example STFT or Mel-Frequency. Research conducted by Facebook AI (now Meta AI) demonstrates a â€œframework for self-supervised learning for speech representationsâ€. In other words, a pre-training phase and architecture which can learn feature representations, and their relationships, by leveraging large amounts of unlabelled, raw audio data.  \n",
    "\n",
    "There are two phases to training: pre-training on unlabelled data, and fine-tuning on a down-stream task. In the original literature the model is fine-tuned for CTC (connectionist temporal classification), which is an ASR task. The consistent modules between pre-training and fine-tuning are what youâ€™d expect to see in a CTC system; it has feature extraction, and an encoder. But, unlike many models of the past, the feature extraction is a convolutional neural network, which makes it trainable. Following that there is a BERT-style encoder where a large convolutional block is used before the first layers, rather than using sinusoidal positional encoding.  \n",
    "\n",
    "### Running on Paperspace\n",
    "\n",
    "The Paperspace environment lets you run this notebook with no set up. To improve your experience we preload datasets and pre-install packages, this can take a few minutes, if you experience errors immediately after starting a session please try restarting the kernel before contacting support. If a problem persists or you want to give us feedback on the content of this notebook, please reach out to through our community of developers using our [slack channel](https://www.graphcore.ai/join-community) or raise a [GitHub issue](https://github.com/gradient-ai/Graphcore-HuggingFace/issues).\n",
    "\n",
    "Requirements:\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3da5633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay         7.3T  6.9T  440G  95% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "tmpfs           252G     0  252G   0% /sys/fs/cgroup\r\n",
      "/dev/md126      7.3T  6.9T  440G  95% /datasets\r\n",
      "tmpfs           252G  1.3M  252G   1% /dev/shm\r\n",
      "tmpfs           252G     0  252G   0% /proc/acpi\r\n",
      "tmpfs           252G     0  252G   0% /proc/scsi\r\n",
      "tmpfs           252G     0  252G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77f3dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1360 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3299 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2554 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1064 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2416 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2820 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Fetched 27.1 MB in 1s (18.9 MB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "71 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libflac8 libvorbisenc2\n",
      "The following NEW packages will be installed:\n",
      "  libflac8 libsndfile1 libvorbisenc2\n",
      "0 upgraded, 3 newly installed, 0 to remove and 71 not upgraded.\n",
      "Need to get 344 kB of archives.\n",
      "After this operation, 1554 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libflac8 amd64 1.3.3-1ubuntu0.1 [103 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libvorbisenc2 amd64 1.3.6-2ubuntu1 [70.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsndfile1 amd64 1.0.28-7ubuntu0.1 [170 kB]\n",
      "Fetched 344 kB in 0s (6977 kB/s)\n",
      "Selecting previously unselected package libflac8:amd64.\r\n",
      "(Reading database ... \r\n",
      "(Reading database ... 5%\r\n",
      "(Reading database ... 10%\r\n",
      "(Reading database ... 15%\r\n",
      "(Reading database ... 20%\r\n",
      "(Reading database ... 25%\r\n",
      "(Reading database ... 30%\r\n",
      "(Reading database ... 35%\r\n",
      "(Reading database ... 40%\r\n",
      "(Reading database ... 45%\r\n",
      "(Reading database ... 50%\r\n",
      "(Reading database ... 55%\r\n",
      "(Reading database ... 60%\r\n",
      "(Reading database ... 65%\r\n",
      "(Reading database ... 70%\r\n",
      "(Reading database ... 75%\r\n",
      "(Reading database ... 80%\r\n",
      "(Reading database ... 85%\r\n",
      "(Reading database ... 90%\r\n",
      "(Reading database ... 95%\r\n",
      "(Reading database ... 100%\r\n",
      "(Reading database ... 58683 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libflac8_1.3.3-1ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking libflac8:amd64 (1.3.3-1ubuntu0.1) ...\r\n",
      "Selecting previously unselected package libvorbisenc2:amd64.\r\n",
      "Preparing to unpack .../libvorbisenc2_1.3.6-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libvorbisenc2:amd64 (1.3.6-2ubuntu1) ...\r\n",
      "Selecting previously unselected package libsndfile1:amd64.\r\n",
      "Preparing to unpack .../libsndfile1_1.0.28-7ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking libsndfile1:amd64 (1.0.28-7ubuntu0.1) ...\r\n",
      "Setting up libflac8:amd64 (1.3.3-1ubuntu0.1) ...\r\n",
      "Setting up libvorbisenc2:amd64 (1.3.6-2ubuntu1) ...\r\n",
      "Setting up libsndfile1:amd64 (1.0.28-7ubuntu0.1) ...\r\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt update\n",
    "apt-get install libsndfile1 -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab567ea7",
   "metadata": {},
   "source": [
    "In order to improve usability and support for future users, Graphcore would like to collect information about the\n",
    "applications and code being run in this notebook. The following information will be anonymised before being sent to Graphcore:\n",
    "\n",
    "- User progression through the notebook\n",
    "- Notebook details: number of cells, code being run and the output of the cells\n",
    "- Environment details\n",
    "\n",
    "You can disable logging at any time by running `%unload_ext gc_logger` from any cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fd2452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable (from -r requirements.txt (line 7))\n",
      "  Cloning https://github.com/graphcore/examples-utils (to revision latest_stable) to /tmp/pip-install-i82n65wg/examples-utils_2611845749454411bbefb71a05438f9e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/graphcore/examples-utils /tmp/pip-install-i82n65wg/examples-utils_2611845749454411bbefb71a05438f9e\n",
      "  Running command git checkout -q 40c62e6646db8f9d60d1707a61204c95a15c7ccb\n",
      "  Resolved https://github.com/graphcore/examples-utils to commit 40c62e6646db8f9d60d1707a61204c95a15c7ccb\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting optimum-graphcore==0.6.1 (from -r requirements.txt (line 1))\n",
      "  Downloading optimum_graphcore-0.6.1-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==0.13.1+cpu (from -r requirements.txt (line 3))\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-0.13.1%2Bcpu-cp38-cp38-linux_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting librosa (from -r requirements.txt (line 4))\n",
      "  Downloading librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jiwer (from -r requirements.txt (line 5))\n",
      "  Downloading jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
      "Collecting soundfile (from -r requirements.txt (line 6))\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.25.1 (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting optimum==1.6.1 (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading optimum-1.6.1-py3-none-any.whl (222 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.6/222.6 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers[torch]==0.12.1 (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading diffusers-0.12.1-py3-none-any.whl (604 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m604.0/604.0 kB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typeguard (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading typeguard-4.0.0-py3-none-any.whl (33 kB)\n",
      "Collecting sentencepiece (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow (from optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchaudio==0.13.1+cpu->-r requirements.txt (line 3)) (1.13.1+cpu)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (6.6.0)\n",
      "Collecting filelock (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub>=0.10.0 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting accelerate>=0.11.0 (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs (from optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy (from optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.20.1 (from optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (23.1)\n",
      "Collecting numpy (from diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.13.1->torchaudio==0.13.1+cpu->-r requirements.txt (line 3)) (4.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.25.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (4.65.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting audioread>=2.1.9 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn>=0.20.0 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=0.14 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from librosa->-r requirements.txt (line 4)) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading numba-0.57.1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pooch<1.7,>=1.0 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soxr>=0.3.2 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading soxr-0.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lazy-loader>=0.1 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->-r requirements.txt (line 4))\n",
      "  Downloading msgpack-1.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click<9.0.0,>=8.1.3 (from jiwer->-r requirements.txt (line 5))\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz==2.13.7 (from jiwer->-r requirements.txt (line 5))\n",
      "  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile->-r requirements.txt (line 6)) (1.15.1)\n",
      "Requirement already satisfied: awscli>=1.24.10 in /usr/local/lib/python3.8/dist-packages (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (1.27.143)\n",
      "Collecting boto3>=1.26 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading boto3-1.26.161-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cppimport>=22.07.17 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading cppimport-22.8.2.tar.gz (26 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gitpython>=3.1 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ipynbname>=2021.3.2 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading ipynbname-2023.1.0.0-py3-none-any.whl (4.3 kB)\n",
      "Requirement already satisfied: nbformat>=5.7.3 in /usr/local/lib/python3.8/dist-packages (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (5.9.0)\n",
      "Collecting psutil>=5.7.0 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting simple-parsing==0.0.19.post1 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading simple_parsing-0.0.19.post1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect (from simple-parsing==0.0.19.post1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: botocore==1.29.143 in /usr/local/lib/python3.8/dist-packages (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (1.29.143)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in /usr/local/lib/python3.8/dist-packages (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.16)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.6.1)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /usr/local/lib/python3.8/dist-packages (from awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from botocore==1.29.143->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore==1.29.143->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/lib/python3/dist-packages (from botocore==1.29.143->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (1.25.8)\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting boto3>=1.26 (from examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading boto3-1.26.160-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.159-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Downloading boto3-1.26.158-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.157-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.9/135.9 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.156-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.155-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.154-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading boto3-1.26.153-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.152-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.151-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.150-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.149-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading boto3-1.26.148-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.147-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.146-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.145-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.144-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading boto3-1.26.143-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 6)) (2.21)\n",
      "Collecting mako (from cppimport>=22.07.17->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybind11 (from cppimport>=22.07.17->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.3/222.3 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython>=3.1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (5.5.6)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (2.17.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (5.3.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (5.9.0)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.51.0->librosa->-r requirements.txt (line 4))\n",
      "  Downloading llvmlite-0.40.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting appdirs>=1.3.0 (from pooch<1.7,>=1.0->librosa->-r requirements.txt (line 4))\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->librosa->-r requirements.txt (line 4))\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading pandas-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp (from datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (3.15.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.19.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->diffusers[torch]==0.12.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<4.8,>=3.1.2->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.5.0)\n",
      "INFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers[sentencepiece]>=4.20.1 (from optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of transformers[sentencepiece] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.2 (from transformers==4.25.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading protobuf-3.20.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (7.16.3)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (8.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (6.3.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat>=5.7.3->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (3.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from mako->cppimport>=22.07.17->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1)) (2023.3)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19 (from sympy->optimum==1.6.1->optimum-graphcore==0.6.1->-r requirements.txt (line 1))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect->simple-parsing==0.0.19.post1->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7))\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (67.8.0)\n",
      "Requirement already satisfied: jedi<=0.17.2,>=0.10 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.17.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (3.0.38)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (2.15.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.29.143->awscli>=1.24.10->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-client->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (25.1.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from jedi<=0.17.2,>=0.10->ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython>=5.0.0->ipykernel->ipynbname>=2021.3.2->examples-utils[common]@ git+https://github.com/graphcore/examples-utils@latest_stable->-r requirements.txt (line 7)) (0.7.0)\n",
      "Building wheels for collected packages: audioread, cppimport, examples-utils\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23702 sha256=52525ec44eeda0f8e69f68504794f3f0e20211034c6da22627a8722d1cddc700\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/ed/be/49df2538fca496690a024a4374455584d65c2afd6fc3d6e9c7\n",
      "  Building wheel for cppimport (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cppimport: filename=cppimport-22.8.2-py3-none-any.whl size=17499 sha256=94367ee009b0d10321b4ba793d89ee03b3302ce4e1aa39fd014407bfd9f71e40\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/d2/91/94b56e3d502b51c6fb096384d6257632cde02f9a9df23f20c1\n",
      "  Building wheel for examples-utils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for examples-utils: filename=examples_utils-0.1.0-py3-none-any.whl size=75817 sha256=8d568221dcc4840fecd965d4bc91f6e570567763a68fb062263ee88648253e21\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rlbove_y/wheels/7b/ef/40/036b06152d165a6d3c2ade41a7f53c22e31a76a515dba9b89b\n",
      "Successfully built audioread cppimport examples-utils\n",
      "Installing collected packages: tokenizers, sentencepiece, msgpack, mpmath, appdirs, xxhash, tzdata, threadpoolctl, sympy, smmap, regex, rapidfuzz, pybind11, psutil, protobuf, pillow, numpy, mypy-extensions, multidict, mako, llvmlite, lazy-loader, joblib, humanfriendly, fsspec, frozenlist, filelock, dill, click, audioread, async-timeout, yarl, typing-inspect, typeguard, torchaudio, soxr, soundfile, scipy, pyarrow, pooch, pandas, numba, multiprocess, jiwer, huggingface-hub, gitdb, cppimport, coloredlogs, aiosignal, accelerate, transformers, simple-parsing, scikit-learn, gitpython, diffusers, aiohttp, librosa, ipynbname, boto3, optimum, examples-utils, datasets, optimum-graphcore\n",
      "Successfully installed accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.2 audioread-3.0.0 boto3-1.26.143 click-8.1.3 coloredlogs-15.0.1 cppimport-22.8.2 datasets-2.13.1 diffusers-0.12.1 dill-0.3.6 examples-utils-0.1.0 filelock-3.12.2 frozenlist-1.3.3 fsspec-2023.6.0 gitdb-4.0.10 gitpython-3.1.31 huggingface-hub-0.15.1 humanfriendly-10.0 ipynbname-2023.1.0.0 jiwer-3.0.2 joblib-1.2.0 lazy-loader-0.2 librosa-0.10.0.post2 llvmlite-0.40.1 mako-1.2.4 mpmath-1.3.0 msgpack-1.0.5 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 numba-0.57.1 numpy-1.23.5 optimum-1.6.1 optimum-graphcore-0.6.1 pandas-2.0.2 pillow-9.5.0 pooch-1.6.0 protobuf-3.20.2 psutil-5.9.5 pyarrow-12.0.1 pybind11-2.10.4 rapidfuzz-2.13.7 regex-2023.6.3 scikit-learn-1.2.2 scipy-1.10.1 sentencepiece-0.1.99 simple-parsing-0.0.19.post1 smmap-5.0.0 soundfile-0.12.1 soxr-0.3.5 sympy-1.12 threadpoolctl-3.1.0 tokenizers-0.13.3 torchaudio-0.13.1+cpu transformers-4.25.1 typeguard-4.0.0 typing-inspect-0.9.0 tzdata-2023.3 xxhash-3.2.0 yarl-1.9.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Loading extensions from ~/.ipython/extensions is deprecated. We recommend managing extensions like any other Python packages, in site-packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "%load_ext graphcore_cloud_tools.notebook_logging.gc_logger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b6ffe81",
   "metadata": {},
   "source": [
    "### Graphcore Hugging Face models\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "Hugging Face models ported to the IPU can be found on the Graphcore organisation page on Hugging Face. \n",
    "\n",
    "### Utility imports\n",
    "We start by importing the utilities that will be used later in the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead2d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, load_metric\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer\n",
    "from optimum.graphcore import IPUTrainingArguments\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForCTC,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Wav2Vec2Processor,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87722337",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ae653f6",
   "metadata": {},
   "source": [
    "Values for machine size and cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a715fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod4\")\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\") + \"/wav2vec2_fine_tuning\"\n",
    "checkpoint_directory = Path(os.getenv(\"PERSISTENT_CHECKPOINT_DIR\", \"/tmp\")) / \"demo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e0058af",
   "metadata": {},
   "source": [
    "## Preparing the LibriSpeech dataset\n",
    "\n",
    "The ğŸ¤—`datasets` library from HuggingFace can be used to load the LibriSpeech dataset, as well as provide a tool to process the data.\n",
    "\n",
    "First we are going to create a `DatasetDict` dictionary to handle our data, and then load the LibriSpeech splits for training and validation. For this notebook we will use `train.100` which is 100 hours of clean training data. Section C of the appendix in the [paper](https://arxiv.org/abs/2006.11477) suggests that fine-tuning a `Base` model can yield 6.1% WER without an additional language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "186b6991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b890be9cafd7423db6311cb82e42fea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bf065b658f40f3b3767f23484730c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaffba9675a4f29be1cd42fab36f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset librispeech_asr/clean to /root/.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/cff5df6e7955c80a67f80e27e7e655de71c689e2d2364bece785b972acb37fe7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f6ef8b43af43edaed62388158508c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e449d41855a14f2c86a379b5ec3e8759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/338M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69875c9a49664fa5a5278c939ca8ad8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901928f798f44fd6b6e3da9f8b448118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac48e16d633b45fe960c54e061e6443f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.100\")\n",
    "raw_datasets[\"eval\"] = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2772b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72624bf2",
   "metadata": {},
   "source": [
    "### Text normalisation\n",
    "\n",
    "Using the package `map` function, any special characters are removed from the transcription. The resulting transcript is then lower-cased. These two processes mean that the model will not have to learn punctuation and capitalisation, although it may have the ability to do so. This is much easier for the model.\n",
    "\n",
    "There are other situations where text normalisation may be used like converting digits into their text counterpart. This is not performed in this script as LibriSpeech already has the text counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9248bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = \"\".join([\",\", \"?\", \".\", \"!\", \"-\", \"\\;\", \"\\:\", \"\\\"\", \"â€œ\", \"%\", \"â€˜\", \"â€\", \"ï¿½\"])\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    if chars_to_ignore_regex is not None:\n",
    "        batch[\"target_text\"] = re.sub(chars_to_ignore_regex, \"\", batch[text_column_name]).lower() + \" \"\n",
    "    else:\n",
    "        batch[\"target_text\"] = batch[text_column_name].lower() + \" \"\n",
    "    return batch\n",
    "\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    remove_special_characters,\n",
    "    remove_columns=[text_column_name],\n",
    "    desc=\"remove special characters from datasets\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "452ab88c",
   "metadata": {},
   "source": [
    "### Create vocabulary and tokenizer\n",
    "\n",
    "We now create a vocabulary from the dataset. This will find all the unique characters from all the normalised text in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e8a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary_from_data(\n",
    "        datasets: DatasetDict,\n",
    "        word_delimiter_token=None,\n",
    "        unk_token=None,\n",
    "        pad_token=None,\n",
    "):\n",
    "    # Given training and test labels create vocabulary\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"target_text\"])\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    vocabs = datasets.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=datasets[\"train\"].column_names,\n",
    "    )\n",
    "\n",
    "    # take union of all unique characters in each dataset\n",
    "    vocab_set = functools.reduce(\n",
    "        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]), vocabs.values()\n",
    "    )\n",
    "\n",
    "    vocab_dict = {v: k for k, v in enumerate(sorted(list(vocab_set)))}\n",
    "\n",
    "    # replace white space with delimiter token\n",
    "    if word_delimiter_token is not None:\n",
    "        vocab_dict[word_delimiter_token] = vocab_dict[\" \"]\n",
    "        del vocab_dict[\" \"]\n",
    "\n",
    "    # add unk and pad token\n",
    "    if unk_token is not None:\n",
    "        vocab_dict[unk_token] = len(vocab_dict)\n",
    "\n",
    "    if pad_token is not None:\n",
    "        vocab_dict[pad_token] = len(vocab_dict)\n",
    "\n",
    "    return vocab_dict\n",
    "\n",
    "\n",
    "word_delimiter_token = \"|\"\n",
    "unk_token = \"[UNK]\"\n",
    "pad_token = \"[PAD]\"\n",
    "\n",
    "vocab_dict = create_vocabulary_from_data(raw_datasets,\n",
    "                                         word_delimiter_token=word_delimiter_token,\n",
    "                                         unk_token=unk_token,\n",
    "                                         pad_token=pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd75ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5e81469",
   "metadata": {},
   "source": [
    "With the vocabulary generated from the normalised trascripts we create a `tokenizer` which is included in the ğŸ¤—`transformers` library. This will later be used to encode text into indexes, and decode indexes into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ab56816",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name_or_path = \"/tmp/wav2vec2-notebook\"\n",
    "\n",
    "vocab_file = os.path.join(tokenizer_name_or_path, \"vocab.json\")\n",
    "\n",
    "if os.path.isfile(vocab_file):\n",
    "    os.remove(vocab_file)\n",
    "\n",
    "os.makedirs(tokenizer_name_or_path, exist_ok=True)\n",
    "\n",
    "with open(vocab_file, \"w\") as file:\n",
    "    json.dump(vocab_dict, file)\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"config\": None,\n",
    "    \"tokenizer_type\": \"wav2vec2\",\n",
    "    \"unk_token\": unk_token,\n",
    "    \"pad_token\": pad_token,\n",
    "    \"word_delimiter_token\": word_delimiter_token,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_auth_token=False, **tokenizer_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca4864ad",
   "metadata": {},
   "source": [
    "Let's look at an example for using the tokenizer. The vocabulary does not contain any digits, so these will be set to `[UNK]`. Remember, any special characters (such as commas) have already been removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f60d6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"wav2vec2 finetuning on ipu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a83390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer(\"wav2vec2 finetuning on ipu\").input_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8373b57",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "Now we generate the feature extraction method for the model and map it across the datasets onto the audio data. In this model we are learning from raw audio signal, so the feature extraction is just used to resample the audio to the rate which the model expects. \n",
    "\n",
    "Afterwards we set the minimum and maximum input lengths in samples. These are set to 2.0 and 15.6 seconds, converted to 32000 and 249600. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8318d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[\"audio\"].sampling_rate\n",
    "if dataset_sampling_rate != 16000:\n",
    "    raw_datasets = raw_datasets.cast_column(\"audio\", datasets.features.Audio(sampling_rate=16000))\n",
    "\n",
    "max_input_length = int(15.6 * feature_extractor.sampling_rate)\n",
    "min_input_length = int(2.0 * feature_extractor.sampling_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ae8003c",
   "metadata": {},
   "source": [
    "### Prepare dataset\n",
    "\n",
    "In this step, both the feature extraction and tokenization are applied to the audio and transcript, respectively. The feature extractor resamples the audio, and the tokenizer will convert the normalised text into indexes.\n",
    "\n",
    "After the map function, the dataset will be filtered by the audio length. If the length of the raw audio is not between 2.0 and 15.6 seconds then it will be removed from the data. The result of filtering is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcb572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    sample = batch[\"audio\"]\n",
    "\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(inputs.input_values[0])\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(batch[\"target_text\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    try:\n",
    "        return length > min_input_length and length < max_input_length\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "vectorized_datasets = raw_datasets.map(prepare_dataset,\n",
    "                                       remove_columns=raw_datasets[\"train\"].column_names,\n",
    "                                       num_proc=8,\n",
    "                                       desc=\"preprocess datasets\")\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(is_audio_in_length_range,\n",
    "                                                 input_columns=[\"input_length\"],\n",
    "                                                 num_proc=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb21c8f1",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "With the dataset prepared, the majority of the processing is complete nearly fit to be sent to the model. The role of the collator is to pad the resampled audio and encoded text to a static size. The padding values for audio will be set to `0.0` but for the indexes they will be `-100` so it's not confused with an index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5a9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.AutoProcessor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"input_values\"] = batch[\"input_values\"].half()\n",
    "\n",
    "        return batch.data\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, pad_to_multiple_of=int(max_input_length),\n",
    "                                           pad_to_multiple_of_labels=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d22f3434",
   "metadata": {},
   "source": [
    "## Preparing the model\n",
    "\n",
    "\n",
    "For the model we are using `wav2vec2-base` from the HuggingFace model-hub. This model has been pretrained only.\n",
    "Some of the defaults options for the model will need to be changed for training:\n",
    "* CTC loss will be normalised by the lengths\n",
    "* There is no masking of the features to be applied so both masks are set to 0.0, the current masking strategy isn't supported on IPU.\n",
    "* The [PAD] index and vocabulary size are later used in the model for the final output layer and CTC-loss.\n",
    "* Epsilon adjusted for FP16 training.\n",
    "\n",
    "\n",
    "The IPU config describes how to parallelise the model across several IPUs. It also includes additional options such as gradient accumulation, device iterations, and memory proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "config.update(\n",
    "    {\n",
    "        \"ctc_loss_reduction\": \"mean\",\n",
    "        \"mask_time_prob\": 0.0,\n",
    "        \"mask_feature_prob\": 0.0,\n",
    "        \"layerdrop\": 0.0,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"layer_norm_eps\": 0.0001,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base\", config=config)\n",
    "\n",
    "ipu_config = IPUConfig.from_pretrained(\"Graphcore/wav2vec2-base-ipu\", executable_cache_dir=executable_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config.layers_per_ipu = [5, 5, 5, 6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02362fcd",
   "metadata": {},
   "source": [
    "Let's set our training hyperparameters using `IPUTrainingArguments`. This subclasses the Hugging Face `TrainingArguments` class, adding parameters specific to the IPU and its execution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = IPUTrainingArguments(output_dir= checkpoint_directory,\n",
    "                                     overwrite_output_dir=True,\n",
    "                                     do_train=True,\n",
    "                                     do_eval=True,\n",
    "                                     evaluation_strategy=\"epoch\",\n",
    "                                     learning_rate=3e-4,\n",
    "                                     num_train_epochs=5.0,\n",
    "                                     adam_epsilon=0.0001,\n",
    "                                     warmup_steps=400,\n",
    "                                     dataloader_drop_last=True,\n",
    "                                     dataloader_num_workers=16,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c665991",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f7e291d",
   "metadata": {},
   "source": [
    "The performance of the model is measured using the WER. This metric takes a predicted string and the correct string and computes an edit distance normalised by the length. For many sentences the sum of the edit distances is normalised by the sum of the lengths. \n",
    "\n",
    "To add this metric to our evaluation we define a `compute_metrics` function and load the metric from the `datasets` package. This is performed once after all the evaluation outputs have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f47ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = {\"wer\": load_metric(\"wer\")}\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cdb5f4f",
   "metadata": {},
   "source": [
    "To train the model, we define a trainer using the `IPUTrainer` class which takes care of compiling the model to run on IPUs, and of performing training and evaluation. The `IPUTrainer` class works just like the HuggingFace `Trainer` class, but takes the additional `ipu_config` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = IPUTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[\"eval\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c44258f",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2fb0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869efa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
