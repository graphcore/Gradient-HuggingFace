# -------- Models --------
tiny: &tiny
  model:
    layers: 2
    hidden_size: 100
    sequence_length: 64
    attention:
      heads: 4
      rotary_dim: 4
    embedding:
      vocab_size: 150

llama2_7b: &llama2_7b
  model:
    layers: 32
    hidden_size: 4096
    intermediate_size: 11008    
    sequence_length: 2048
    attention:
      heads: 32
    embedding:
      vocab_size: 32000
    eps: 1.0e-6

llama2_13b: &llama2_13b
  model:
    layers: 40
    hidden_size: 5120
    intermediate_size: 13824
    sequence_length: 2048
    attention:
      heads: 40
    embedding:
      vocab_size: 32000
    eps: 1.0e-5

# -------------------------

# ------- Execution -------
release:
  tiny:
    <<: *tiny
    execution:
      micro_batch_size: 4
      available_memory_proportion: [ 0.4 ]
      tensor_parallel: 4

  llama2_7b_pod2:
    <<: *llama2_7b
    execution:
      micro_batch_size: 1
      available_memory_proportion: [ 0.1 ]
      tensor_parallel: 2
    
  llama2_7b_pod4:
    <<: *llama2_7b
    execution:
      micro_batch_size: 1
      available_memory_proportion: [ 0.4 ]
      tensor_parallel: 4

  llama2_7b_pod16:
    <<: *llama2_7b
    execution:
      micro_batch_size: 1
      available_memory_proportion: [ 0.4 ]
      tensor_parallel: 16

  llama2_13b_pod4:
    <<: *llama2_13b
    execution:
      micro_batch_size: 1
      available_memory_proportion: [ 0.4 ]
      tensor_parallel: 4
  
  # TODO: Add attention padding to support attention tensor-parallel up to 16
  llama2_13b_pod16:
    <<: *llama2_13b
    execution:
      micro_batch_size: 1
      available_memory_proportion: [ 0.4 ]
      tensor_parallel: 16
      attention_tensor_parallel: 4
    