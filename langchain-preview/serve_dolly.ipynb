{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b832530-546e-4196-9d72-278d6d906e36",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a proof-of-concept for serving [Dolly 2.0 Large Language Model](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) on IPUs, and use  [use it from LangChain](https://python.langchain.com/en/latest/modules/models/llms.html), as a plug-in replacement of any `langchain.llm` objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc104f8-1135-4190-9457-cd19d479deb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T08:29:21.553989Z",
     "iopub.status.busy": "2023-06-01T08:29:21.553706Z",
     "iopub.status.idle": "2023-06-01T08:32:41.408353Z",
     "shell.execute_reply": "2023-06-01T08:32:41.407299Z",
     "shell.execute_reply.started": "2023-06-01T08:29:21.553961Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install flask manifest-ml ngrok\n",
    "!pip install \"optimum-graphcore>=0.6.1, <0.7.0\"\n",
    "# For Dolly 2.0\n",
    "!pip install \"examples-utils[common] @ git+https://github.com/graphcore/examples-utils.git@latest_stable\"\n",
    "!pip install \"git+https://github.com/graphcore/popxl-addons.git@sdk-release-3.2\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8ff0b7d-2479-4cde-b9b4-e1266758a9fc",
   "metadata": {},
   "source": [
    "## Create an HTTP tunnel\n",
    "\n",
    "We create an HTTP tunnel to make the served model accessible from a LanghChain environment running on a different machine. This allows fast and east experiments in a local LangChain notebook while delegating compuationally intensive inference tasks to the IPUs. \n",
    "\n",
    "The tunnel is created using [ngrok](https://ngrok.com/). You will need to sign up to ngrok, get a free authentication token, and store the token in the environment variable `NGROK_AUTHTOKEN`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fedbe-732a-4c36-9784-b6ca8d0c7a92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T08:32:41.410412Z",
     "iopub.status.busy": "2023-06-01T08:32:41.410219Z",
     "iopub.status.idle": "2023-06-01T08:32:44.903469Z",
     "shell.execute_reply": "2023-06-01T08:32:44.902636Z",
     "shell.execute_reply.started": "2023-06-01T08:32:41.410388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Required to start ngrok tunnel in a notebook environment\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ngrok token\n",
    "import os\n",
    "# Insert your ngrok authentication token here:\n",
    "os.environ['NGROK_AUTHTOKEN'] = \"Insert your ngrok authentication token here\"\n",
    "import ngrok\n",
    "\n",
    "# Needed for coroutine's in Notebooks\n",
    "import asyncio\n",
    "loop = asyncio.get_event_loop()\n",
    "\n",
    "tunnel = loop.run_until_complete(ngrok.werkzeug_develop())\n",
    "# tunnel = await start_tunnel()\n",
    "print(f\"\"\"\n",
    "Use the following call in your LangChain:\n",
    "\n",
    "llm = ManifestWrapper(\n",
    "    client=Manifest(\n",
    "        client_name=\"huggingface\",\n",
    "        client_connection=\"{tunnel.url()}\",\n",
    "    ),\n",
    "    llm_kwargs={{\"client_timeout\": 500, \"max_tokens\": 2048}}\n",
    ")\n",
    "\"\"\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb63e59-ea86-4c0f-a473-4d183f5438bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T08:32:44.904770Z",
     "iopub.status.busy": "2023-06-01T08:32:44.904566Z",
     "iopub.status.idle": "2023-06-01T08:32:44.907535Z",
     "shell.execute_reply": "2023-06-01T08:32:44.906947Z",
     "shell.execute_reply.started": "2023-06-01T08:32:44.904751Z"
    }
   },
   "outputs": [],
   "source": [
    "# On ngrok free tier only one active tunne is allowed at one time. This is a problem is the notebook times out (or runs in the background, in a closed page) as ngrok will fail to create a new tunnel.\n",
    "# I aven't been able to find how to kill the old tunnels.  \n",
    "# ngrok.disconnect(\"https://2ba3-38-83-162-251.ngrok-free.app/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "292d8bb7-a6a1-4c92-a788-7f5951896c75",
   "metadata": {},
   "source": [
    "## Pre-compile ML models\n",
    "\n",
    "We compile and load the LLM into the IPU. This can take up to 10 minutes. Once compiled and loaded, the model be used for inference with minimal latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eabb08-e5e6-4c1d-a90b-e45f1a1e0fc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T08:32:44.908521Z",
     "iopub.status.busy": "2023-06-01T08:32:44.908355Z",
     "iopub.status.idle": "2023-06-01T09:00:13.452352Z",
     "shell.execute_reply": "2023-06-01T09:00:13.448286Z",
     "shell.execute_reply.started": "2023-06-01T08:32:44.908504Z"
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_IPUS = 4\n",
    "\n",
    "# HuggingFace Optimum models\n",
    "if False:\n",
    "    # TODO Serve mulitple model\n",
    "    print(\"Creating FLAN T5 pipeline\")\n",
    "    from optimum.graphcore import pipeline\n",
    "\n",
    "    size = {4: \"large\", 16: \"xl\"}\n",
    "    flan_t5_pipeline = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=f\"google/flan-t5-{size[NUMBER_OF_IPUS]}\",\n",
    "        ipu_config=f\"Graphcore/t5-{size[NUMBER_OF_IPUS]}-ipu\",\n",
    "        max_input_length=896,\n",
    "    )\n",
    "    print(\"Pre-compiling T5\")\n",
    "    r = flan_t5_pipeline(\"precompile this\")\n",
    "    print(r)\n",
    "    print(\"T5 pre-compilation done.\")\n",
    "\n",
    "\n",
    "# PopXL models\n",
    "print(\"Creating Dolly pipeline\")\n",
    "    \n",
    "# Dolly 2.0\n",
    "import sys\n",
    "sys.path.append('../dolly2-instruction-following')\n",
    "from utils.setup import dolly_config_setup\n",
    "import api\n",
    "\n",
    "sequence_length = 2048  # max 2048\n",
    "micro_batch_size = 1\n",
    "\n",
    "config_name = \"dolly_pod4\" if NUMBER_OF_IPUS == 4 else \"dolly_pod16\"\n",
    "config, *_ = dolly_config_setup(\"../dolly2-instruction-following/config/inference.yml\", \"release\", config_name)\n",
    "\n",
    "dolly_pipeline = api.DollyPipeline(\n",
    "    config, sequence_length=sequence_length, micro_batch_size=micro_batch_size\n",
    ")\n",
    "\n",
    "print(\"Pre-compiling Dolly\")\n",
    "r = dolly_pipeline(\"precompile this\")\n",
    "print(r)\n",
    "print(\"Dolly pre-compilation done.\")\n",
    "\n",
    "ML_MODELS_PROVIDED = {\n",
    "    'ipu-dolly2' : dolly_pipeline\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc62554-078f-4674-856f-7e80b5697043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343b802-e265-4792-a50d-2ebb7c0fdf21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T09:00:13.462518Z",
     "iopub.status.busy": "2023-06-01T09:00:13.462019Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Flask app.\"\"\"\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import pkg_resources\n",
    "from flask import Flask, Response, request\n",
    "\n",
    "from manifest.api.response import ModelResponse\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "app = Flask(__name__)  # define app using Flask\n",
    "\n",
    "# Will be global\n",
    "\n",
    "PORT = int(os.environ.get(\"FLASK_PORT\", 5000))\n",
    "MODEL_CONSTRUCTORS = {\n",
    "    # \"huggingface\": TextGenerationModel,\n",
    "}\n",
    "    \n",
    "@app.route(\"/completions\", methods=[\"POST\"])\n",
    "def completions() -> Response:\n",
    "    \"\"\"Get completions for generation.\"\"\"\n",
    "    prompt = request.json[\"prompt\"]\n",
    "    del request.json[\"prompt\"]\n",
    "    generation_args = request.json\n",
    "    \n",
    "    print(\"generation args:\\n\" + str(request.json))\n",
    "\n",
    "    if not isinstance(prompt, (str, list)):\n",
    "        raise ValueError(\"Prompt must be a str or list of str\")\n",
    "    try:\n",
    "        print(f\"** Calling pipeline({prompt})\")\n",
    "        # result_gens = flan_t5(prompt)\n",
    "        result_gens = dolly_pipeline(prompt, temperature=request.json['temperature'], k=request.json['top_k'])\n",
    "        print(\"Results\")\n",
    "        print(f\"r = {r}\")\n",
    "        \n",
    "        results = [\n",
    "                {\"text\": r, \"logprob\": [], \"tokens\": [], \"token_logprobs\": []}\n",
    "                for r in result_gens\n",
    "            ]\n",
    "                # {\"text\": r['generated_text'], \"logprob\": [], \"tokens\": [], \"token_logprobs\": []}\n",
    "        res_type = \"text_completion\"\n",
    "        print(\"*** SENDING RESPONSE\")\n",
    "        # transform the result into the openai format\n",
    "        return Response(\n",
    "            json.dumps(ModelResponse(results, response_type=res_type).__dict__()),\n",
    "            status=200,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return Response(\n",
    "            json.dumps({\"message\": str(e)}),\n",
    "            status=400,\n",
    "        )\n",
    "\n",
    "@app.route(\"/params\", methods=[\"POST\"])\n",
    "def params() -> Dict:\n",
    "    \"\"\"Get model params.\"\"\"\n",
    "    print('/params')\n",
    "    return {\"model_name\": \"google/flan-t5-large\", \"model_path\": \"google/flan-t5-large\"}\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index() -> str:\n",
    "    \"\"\"Get index completion.\"\"\"\n",
    "    return \"Index page\"\n",
    "\n",
    "print(\"* Starting app*\")\n",
    "app.run(host=\"0.0.0.0\", port=PORT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af2359-2e4f-408e-a792-c7d245871088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrok.kill()\n",
    "# tunnel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7091cf-210b-48cb-9fce-c7b39fdb319f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
