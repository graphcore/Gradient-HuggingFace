{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7a2f86",
   "metadata": {},
   "source": [
    "# Speech Transcription on IPUs using Whisper - Inference\n",
    "\n",
    "This notebook demonstrates speech transcription on the IPU using the [Whisper implementation in the Hugging Face Transformers library](https://huggingface.co/spaces/openai/whisper) alongside [Optimum Graphcore](https://github.com/huggingface/optimum-graphcore).\n",
    "\n",
    "Whisper is a versatile speech recognition model that can transcribe speech as well as perform multi-lingual translation and recognition tasks.\n",
    "It was trained on diverse datasets to give human-level speech recognition performance without the need for fine tuning. \n",
    "\n",
    "Optimum Graphcore is the interface between the Hugging Face Transformers library and [Graphcore IPUs](https://www.graphcore.ai/products/ipu).\n",
    "It provides a set of tools enabling model parallelization and loading on IPUs, training and fine-tuning on all the tasks already supported by Transformers while being compatible with the Hugging Face Hub and every model available on it out of the box.\n",
    "\n",
    "> **Hardware requirements:** The Whisper models `whisper-tiny`, `whisper-base` and `whisper-small` can run two replicas on the smallest IPU-POD4 machine. The most capable model, `whisper-large`, will need to use either an IPU-POD16 or a Bow Pod16 machine. Please contact Graphcore if you'd like assistance running model sizes that don't work in this simple example notebook.\n",
    "\n",
    "[![Join our Slack Community](https://img.shields.io/badge/Slack-Join%20Graphcore's%20Community-blue?style=flat-square&logo=slack)](https://www.graphcore.ai/join-community)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458fecc",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "\n",
    "In order to run this notebook you will need to be in an environment with the Poplar SDK installed and enabled. This is done by default on Paperspace. If you are not using Paperspace, refer to the [getting started guide](https://docs.graphcore.ai/en/latest/getting-started.html#getting-started) for your system for a description of how to set this up.\n",
    "\n",
    "We also need the Optimum Graphcore interface to the Hugging Face Transformers library, and there are a few extra dependencies we need to be able to handle audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde99b10-e2d2-4787-877f-fb120e327ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"optimum-graphcore>=0.6, <0.7\"\n",
    "%pip install soundfile==0.12.1 librosa==0.10.0.post2 tokenizers==0.12.1\n",
    "%pip install matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install examples-utils[common]@git+https://github.com/graphcore/examples-utils@latest_stable\n",
    "from examples_utils import notebook_logging\n",
    "%load_ext gc_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08888a86",
   "metadata": {},
   "source": [
    "## Running Whisper on the IPU\n",
    "\n",
    "We start by importing the required modules, some of which are needed to configure the IPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6efd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "from datasets import load_dataset\n",
    "import matplotlib\n",
    "import librosa\n",
    "import IPython\n",
    "import random\n",
    "import os\n",
    "\n",
    "# IPU-specific imports\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "\n",
    "# HF-related imports\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d8d54",
   "metadata": {},
   "source": [
    "The Whisper model is available on Hugging Face in several sizes, from `whisper-tiny` with 39M parameters to `whisper-large` with 1550M parameters.\n",
    "\n",
    "We download `whisper-tiny` which we will run using two IPUs.\n",
    "The [Whisper architecture](https://openai.com/research/whisper) is an encoder-decoder Transformer, with the audio split into 30-second chunks.\n",
    "For simplicity one IPU is used for the encoder part of the graph and another for the decoder part.\n",
    "The `IPUConfig` object helps to configure the model to be pipelined across the IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d72f3-cbd6-462f-9741-1726d412c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = \"openai/whisper-tiny.en\"\n",
    "\n",
    "# Instantiate processor and model\n",
    "processor = WhisperProcessor.from_pretrained(model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_spec)\n",
    "\n",
    "# Adapt whisper-tiny to run on the IPU\n",
    "ipu_config = IPUConfig(ipus_per_replica=2, executable_cache_dir=\"./exe_cache\")\n",
    "pipelined_model = to_pipelined(model, ipu_config)\n",
    "pipelined_model = pipelined_model.parallelize(for_generation=True).half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99620b",
   "metadata": {},
   "source": [
    "Now we can load the dataset and process an example audio file.\n",
    "If precompiled models are not available, then the first run of the model triggers two graph compilations.\n",
    "This means that our first test transcription could take a minute or two to run, but subsequent runs will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab692b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and read an example sound file\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "test_sample = ds[2]\n",
    "sample_rate = test_sample['audio']['sampling_rate']\n",
    "\n",
    "def transcribe(data, rate):\n",
    "    input_features = processor(data, return_tensors=\"pt\", sampling_rate=rate).input_features.half()\n",
    "\n",
    "    # This triggers a compilation, unless a precompiled model is available.\n",
    "    sample_output = pipelined_model.generate(input_features, max_length=448, min_length=3)\n",
    "    transcription = processor.batch_decode(sample_output, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "test_transcription = transcribe(test_sample[\"audio\"][\"array\"], sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59411d",
   "metadata": {},
   "source": [
    "In the next cell, we compare the expected text from the dataset with the transcribed result from the model.\n",
    "There will typically be some small differences, but even `whisper-tiny` does a great job! It even adds punctuation.\n",
    "\n",
    "You can listen to the audio and compare the model result yourself using the controls below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17947b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Expected: {test_sample['text']}\\n\")\n",
    "print(f\"Transcribed: {test_transcription}\")\n",
    "\n",
    "matplotlib.pyplot.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(test_sample[\"audio\"][\"array\"], sr=sample_rate)\n",
    "IPython.display.Audio(test_sample[\"audio\"][\"array\"], rate=sample_rate)\n",
    "print(sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f7821-1ddb-425e-995e-a9f084c7ff0b",
   "metadata": {},
   "source": [
    "The model only needs to be compiled once. Subsequent inferences will be much faster.\n",
    "In the cell below, we repeat the exercise but with a random example from the dataset.\n",
    "\n",
    "You might like to re-run this next cell multiple times to get different comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9ca3-a932-4e66-97c7-8ffe98d00bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, ds.num_rows - 1)\n",
    "data = ds[idx][\"audio\"][\"array\"]\n",
    "\n",
    "print(f\"Example #{idx}\\n\")\n",
    "print(f\"Expected: {ds[idx]['text']}\\n\")\n",
    "print(f\"Transcribed: {transcribe(data, sample_rate)}\")\n",
    "\n",
    "matplotlib.pyplot.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(data, sr=sample_rate)\n",
    "IPython.display.Audio(data, rate=sample_rate, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625a6bf",
   "metadata": {},
   "source": [
    "Finally, we detach the process from the IPUs when we are done to make the IPUs available to other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelined_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4e580-703e-4329-9dba-808a3a1096c8",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The `whisper-tiny` model used here is very fast for inference and so cheap to run, but its accuracy can be improved.\n",
    "The `whisper-base` and `whisper-small` models have 74M and 244M parameters respectively (compared to just 39M for `whisper-tiny`). You can try out `whisper-base` and `whisper-small` by changing `model_spec = \"openai/whisper-tiny.en\"` (at the beginning of this notebook) to `model_spec = \"openai/whisper-base.en\"` or `model_spec = \"openai/whisper-small.en\"` respectively.\n",
    "\n",
    "Larger models and multilingual models are also available.\n",
    "To access the multilingual models, remove the `.en` from the checkpoint name. Note however that the multilingual models are slightly less accurate for this English transcription task but they can be used for transcribing other languages or for translating to English.\n",
    "\n",
    "The largest models have 1550M parameters and won't fit with our simple two-IPU pipeline.\n",
    "To run these you will need more than the IPU-POD4. On Paperspace, this is available using either an IPU-POD16 or a Bow Pod16 machine. Please contact Graphcore if you need assistance running these larger models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff7629",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrated using Whisper for speech recognition and transcription on the IPU.\n",
    "We used the Optimum Graphcore package to interface between the IPU and the Hugging Face Transformers library. This meant that only a few lines of code were needed to get this state-of-the-art automated speech recognition model running on IPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a4c55-4196-4b76-8f33-409c5cbbd3b5",
   "metadata": {},
   "source": [
    "# Bonus: Deploy whisper in one-click\n",
    "We can now deploy our own Whisper API hosted in Paperspace Deployment!\n",
    "Find a step-by-step guide in [deploy-whisper-api notebook](deploy-whisper-api.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b753de3-7089-4eeb-bd6b-1e3c1b3cf29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from widgets import deploy,shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbee4a5f-7e99-4492-9440-bc218a759f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bb7c611ce748d9b1b68c7d74b0c4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Paperspace project Id', description='Project ID'), Password(description='Token:', pâ€¦"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f2ce3-009b-4972-b672-9d16d140c8f9",
   "metadata": {},
   "source": [
    "**Once you are done, you can close the server to free the resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "790b9abd-f8a1-427a-b2bb-505ff4106d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab7849e1d7c43d08c9cd27ff08dfb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(description='Shutdown', style=ButtonStyle()), Output()))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
